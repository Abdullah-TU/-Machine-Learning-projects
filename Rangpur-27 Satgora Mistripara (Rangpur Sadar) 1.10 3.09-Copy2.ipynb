{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6d594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51831ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'D:\\Jupyter\\Ground water level prediction(Towfiq Sir)\\final_data_updated.xlsx')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da445da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataFrame_Checker import DataFrameChecker\n",
    "\n",
    "#  an instance of DataFrameChecker\n",
    "checker = DataFrameChecker(df)\n",
    "\n",
    "\n",
    "# Called the checking functions\n",
    "checker.check_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checker.check_missing_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf276e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1878bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "cols_to_convert = df.columns.difference(['Date'])\n",
    "\n",
    "df[cols_to_convert] = df[cols_to_convert].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Rangpur-27 Satgora Mistripara (Rangpur Sadar)\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76beffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d.%m.%Y')\n",
    "\n",
    "df_train = df.loc[df['Date'].dt.year <= 2012]\n",
    "df_test = df.loc[df['Date'].dt.year >= 2013]\n",
    "\n",
    "X_train = df_train.drop(columns=['Rangpur-27 Satgora Mistripara (Rangpur Sadar)', 'Date']).to_numpy()\n",
    "y_train = df_train['Rangpur-27 Satgora Mistripara (Rangpur Sadar)'].to_numpy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from tbats import TBATS\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "\n",
    "\n",
    "random_state = 42 \n",
    "\n",
    "# Initialize different base models\n",
    "linear_regression = LinearRegression()\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "random_forest = RandomForestRegressor(random_state=random_state)\n",
    "ridge = Ridge()\n",
    "knn = KNeighborsRegressor()\n",
    "gaussian_process = GaussianProcessRegressor()\n",
    "poly_reg = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
    "poly_kernel = make_pipeline(StandardScaler(), PolynomialFeatures(3), LinearRegression())\n",
    "rbf_kernel = make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=None, n_restarts_optimizer=10, random_state=random_state))\n",
    "gpr = GaussianProcessRegressor(random_state=random_state)\n",
    "weighted_knn = KNeighborsRegressor(weights='distance')\n",
    "lightgbm = LGBMRegressor(random_state=random_state)\n",
    "catboost = CatBoostRegressor(random_state=random_state, verbose=0)\n",
    "xgb_model = XGBRegressor()\n",
    "lgb_regressor = lgb.LGBMRegressor()\n",
    "gbr = GradientBoostingRegressor(random_state=random_state)\n",
    "abr = AdaBoostRegressor(random_state=random_state)\n",
    "\n",
    "\n",
    "# Train each base model on the training set\n",
    "linear_regression.fit(X_train, y_train)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "random_forest.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "gaussian_process.fit(X_train, y_train)\n",
    "poly_reg.fit(X_train, y_train)\n",
    "poly_kernel.fit(X_train, y_train)\n",
    "rbf_kernel.fit(X_train, y_train)\n",
    "gpr.fit(X_train, y_train)\n",
    "weighted_knn.fit(X_train, y_train)\n",
    "lightgbm.fit(X_train, y_train)\n",
    "catboost.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "lgb_regressor.fit(X_train, y_train)\n",
    "gbr.fit(X_train, y_train)\n",
    "abr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Train and evaluate Linear Regression\n",
    "linear_pred_val = linear_regression.predict(X_val)\n",
    "linear_rmse_val = mean_squared_error(y_val, linear_pred_val, squared=False)\n",
    "linear_mae_val = mean_absolute_error(y_val, linear_pred_val)\n",
    "linear_r2_val = r2_score(y_val, linear_pred_val)\n",
    "\n",
    "# Train and evaluate Decision Tree\n",
    "dt_pred_val = decision_tree.predict(X_val)\n",
    "dt_rmse_val = mean_squared_error(y_val, dt_pred_val, squared=False)\n",
    "dt_mae_val = mean_absolute_error(y_val, dt_pred_val)\n",
    "dt_r2_val = r2_score(y_val, dt_pred_val)\n",
    "\n",
    "# Train and evaluate Random Forest\n",
    "rf_pred_val = random_forest.predict(X_val)\n",
    "rf_rmse_val = mean_squared_error(y_val, rf_pred_val, squared=False)\n",
    "rf_mae_val = mean_absolute_error(y_val, rf_pred_val)\n",
    "rf_r2_val = r2_score(y_val, rf_pred_val)\n",
    "\n",
    "# Train and evaluate Ridge Regression\n",
    "ridge_pred_val = ridge.predict(X_val)\n",
    "ridge_rmse_val = mean_squared_error(y_val, ridge_pred_val, squared=False)\n",
    "ridge_mae_val = mean_absolute_error(y_val, ridge_pred_val)\n",
    "ridge_r2_val = r2_score(y_val, ridge_pred_val)\n",
    "\n",
    "\n",
    "# Train and evaluate K-Nearest Neighbors\n",
    "knn_pred_val = knn.predict(X_val)\n",
    "knn_rmse_val = mean_squared_error(y_val, knn_pred_val, squared=False)\n",
    "knn_mae_val = mean_absolute_error(y_val, knn_pred_val)\n",
    "knn_r2_val = r2_score(y_val, knn_pred_val)\n",
    "\n",
    "# Train and evaluate Gaussian Process\n",
    "gp_pred_val = gaussian_process.predict(X_val)\n",
    "gp_rmse_val = mean_squared_error(y_val, gp_pred_val, squared=False)\n",
    "gp_mae_val = mean_absolute_error(y_val, gp_pred_val)\n",
    "gp_r2_val = r2_score(y_val, gp_pred_val)\n",
    "\n",
    "\n",
    "# Train and evaluate Polynomial Regression\n",
    "poly_reg_pred_val = poly_reg.predict(X_val)\n",
    "poly_reg_rmse_val = mean_squared_error(y_val, poly_reg_pred_val, squared=False)\n",
    "poly_reg_mae_val = mean_absolute_error(y_val, poly_reg_pred_val)\n",
    "poly_reg_r2_val = r2_score(y_val, poly_reg_pred_val)\n",
    "\n",
    "# Train and evaluate Poly Kernel\n",
    "poly_kernel_pred_val = poly_kernel.predict(X_val)\n",
    "poly_kernel_rmse_val = mean_squared_error(y_val, poly_kernel_pred_val, squared=False)\n",
    "poly_kernel_mae_val = mean_absolute_error(y_val, poly_kernel_pred_val)\n",
    "poly_kernel_r2_val = r2_score(y_val, poly_kernel_pred_val)\n",
    "\n",
    "# Train and evaluate RBF Kernel\n",
    "rbf_kernel_pred_val = rbf_kernel.predict(X_val)\n",
    "rbf_kernel_rmse_val = mean_squared_error(y_val, rbf_kernel_pred_val, squared=False)\n",
    "rbf_kernel_mae_val = mean_absolute_error(y_val, rbf_kernel_pred_val)\n",
    "rbf_kernel_r2_val = r2_score(y_val, rbf_kernel_pred_val)\n",
    "\n",
    "# Train and evaluate Gaussian Process Regression\n",
    "gpr_pred_val = gpr.predict(X_val)\n",
    "gpr_rmse_val = mean_squared_error(y_val, gpr_pred_val, squared=False)\n",
    "gpr_mae_val = mean_absolute_error(y_val, gpr_pred_val)\n",
    "gpr_r2_val = r2_score(y_val, gpr_pred_val)\n",
    "\n",
    "# Train and evaluate Weighted K-NN\n",
    "wknn_pred_val = weighted_knn.predict(X_val)\n",
    "wknn_rmse_val = mean_squared_error(y_val, wknn_pred_val, squared=False)\n",
    "wknn_mae_val = mean_absolute_error(y_val, wknn_pred_val)\n",
    "wknn_r2_val = r2_score(y_val, wknn_pred_val)\n",
    "\n",
    "# Train and evaluate Gradient Boosting Regressor\n",
    "gbr_pred_val = gbr.predict(X_val)\n",
    "gbr_rmse_val = mean_squared_error(y_val, gbr_pred_val, squared=False)\n",
    "gbr_mae_val = mean_absolute_error(y_val, gbr_pred_val)\n",
    "gbr_r2_val = r2_score(y_val, gbr_pred_val)\n",
    "\n",
    "# Train and evaluate AdaBoost Regressor\n",
    "abr_pred_val = abr.predict(X_val)\n",
    "abr_rmse_val = mean_squared_error(y_val, abr_pred_val, squared=False)\n",
    "abr_mae_val = mean_absolute_error(y_val, abr_pred_val)\n",
    "abr_r2_val = r2_score(y_val, abr_pred_val)\n",
    "\n",
    "# Train and evaluate LightGBM Regressor\n",
    "lightgbm_pred_val = lightgbm.predict(X_val)\n",
    "lightgbm_rmse_val = mean_squared_error(y_val, lightgbm_pred_val, squared=False)\n",
    "lightgbm_mae_val = mean_absolute_error(y_val, lightgbm_pred_val)\n",
    "lightgbm_r2_val = r2_score(y_val, lightgbm_pred_val)\n",
    "\n",
    "# Train and evaluate CatBoost Regressor\n",
    "catboost_pred_val = catboost.predict(X_val)\n",
    "catboost_rmse_val = mean_squared_error(y_val, catboost_pred_val, squared=False)\n",
    "catboost_mae_val = mean_absolute_error(y_val, catboost_pred_val)\n",
    "catboost_r2_val = r2_score(y_val, catboost_pred_val)\n",
    "\n",
    "# Train and evaluate XGBoost\n",
    "xgb_pred_val = xgb_model.predict(X_val)\n",
    "xgb_rmse_val = mean_squared_error(y_val, xgb_pred_val, squared=False)\n",
    "xgb_mae_val = mean_absolute_error(y_val, xgb_pred_val)\n",
    "xgb_r2_val = r2_score(y_val, xgb_pred_val)\n",
    "\n",
    "# Train and evaluate LightGBM Regressor\n",
    "lgb_pred_val = lgb_regressor.predict(X_val)\n",
    "lgb_rmse_val = mean_squared_error(y_val, lgb_pred_val, squared=False)\n",
    "lgb_mae_val = mean_absolute_error(y_val, lgb_pred_val)\n",
    "lgb_r2_val = r2_score(y_val, lgb_pred_val)\n",
    "\n",
    "\n",
    "# Print evaluation metrics for each model on the validation set\n",
    "\n",
    "# Print the evaluation metrics for each model\n",
    "print(\"Linear Regression RMSE:\", linear_rmse_val)\n",
    "print(\"Decision Tree RMSE:\", dt_rmse_val)\n",
    "print(\"Random Forest RMSE:\", rf_rmse_val)\n",
    "print(\"Ridge Regression RMSE:\", ridge_rmse_val)\n",
    "print(\"K-Nearest Neighbors RMSE:\", knn_rmse_val)\n",
    "print(\"Gaussian Process RMSE:\", gp_rmse_val)\n",
    "print(\"Polynomial Regression RMSE:\", poly_reg_rmse_val)\n",
    "print(\"Poly Kernel RMSE:\", poly_kernel_rmse_val)\n",
    "print(\"RBF Kernel RMSE:\", rbf_kernel_rmse_val)\n",
    "print(\"Gaussian Process Regression RMSE:\", gpr_rmse_val)\n",
    "print(\"Weighted K-NN RMSE:\", wknn_rmse_val)\n",
    "print(\"Gradient Boosting Regressor RMSE:\", gbr_rmse_val)\n",
    "print(\"AdaBoost Regressor RMSE:\", abr_rmse_val)\n",
    "print(\"LightGBM Regressor RMSE:\", lightgbm_rmse_val)\n",
    "print(\"CatBoost Regressor RMSE:\", catboost_rmse_val)\n",
    "print(\"XGBoost RMSE:\", xgb_rmse_val)\n",
    "print(\"LightGBM Regressor RMSE:\", lgb_rmse_val)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Linear Regression MAE:\", linear_mae_val)\n",
    "print(\"Decision Tree MAE:\", dt_mae_val)\n",
    "print(\"Random Forest MAE:\", rf_mae_val)\n",
    "print(\"Ridge Regression MAE:\", ridge_mae_val)\n",
    "print(\"K-Nearest Neighbors MAE:\", knn_mae_val)\n",
    "print(\"Gaussian Process MAE:\", gp_mae_val)\n",
    "print(\"Polynomial Regression MAE:\", poly_reg_mae_val)\n",
    "print(\"Poly Kernel MAE:\", poly_kernel_mae_val)\n",
    "print(\"RBF Kernel MAE:\", rbf_kernel_mae_val)\n",
    "print(\"Gaussian Process Regression MAE:\", gpr_mae_val)\n",
    "print(\"Weighted K-NN MAE:\", wknn_mae_val)\n",
    "print(\"Gradient Boosting Regressor MAE:\", gbr_mae_val)\n",
    "print(\"AdaBoost Regressor MAE:\", abr_mae_val)\n",
    "print(\"LightGBM Regressor MAE:\", lightgbm_mae_val)\n",
    "print(\"CatBoost Regressor MAE:\", catboost_mae_val)\n",
    "print(\"XGBoost MAE:\", xgb_mae_val)\n",
    "print(\"LightGBM Regressor MAE:\", lgb_mae_val)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Linear Regression R-squared:\", linear_r2_val)\n",
    "print(\"Decision Tree R-squared:\", dt_r2_val)\n",
    "print(\"Random Forest R-squared:\", rf_r2_val)\n",
    "print(\"Ridge Regression R-squared:\", ridge_r2_val)\n",
    "print(\"K-Nearest Neighbors R-squared:\", knn_r2_val)\n",
    "print(\"Gaussian Process R-squared:\", gp_r2_val)\n",
    "print(\"Polynomial Regression R-squared:\", poly_reg_r2_val)\n",
    "print(\"Poly Kernel R-squared:\", poly_kernel_r2_val)\n",
    "print(\"RBF Kernel R-squared:\", rbf_kernel_r2_val)\n",
    "print(\"Gaussian Process Regression R-squared:\", gpr_r2_val)\n",
    "print(\"Weighted K-NN R-squared:\", wknn_r2_val)\n",
    "print(\"Gradient Boosting Regressor R-squared:\", gbr_r2_val)\n",
    "print(\"AdaBoost Regressor R-squared:\", abr_r2_val)\n",
    "print(\"LightGBM Regressor R-squared:\", lightgbm_r2_val)\n",
    "print(\"CatBoost Regressor R-squared:\", catboost_r2_val)\n",
    "print(\"XGBoost R-squared:\", xgb_r2_val)\n",
    "print(\"LightGBM Regressor R-squared:\", lgb_r2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the range of neighbors to consider\n",
    "num_neighbors = 5  # You can adjust this value\n",
    "\n",
    "# Initialize LWLR model\n",
    "lwlr = KNeighborsRegressor(n_neighbors=num_neighbors, weights='distance')\n",
    "\n",
    "# Train the LWLR model\n",
    "lwlr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "lwlr_pred = lwlr.predict(X_val)\n",
    "\n",
    "# Calculate metrics for LWLR\n",
    "lwlr_rmse = mean_squared_error(y_val, lwlr_pred, squared=False)\n",
    "lwlr_mae = mean_absolute_error(y_val, lwlr_pred)\n",
    "lwlr_r2 = r2_score(y_val, lwlr_pred)\n",
    "\n",
    "# Print the evaluation metrics for LWLR\n",
    "print(\"LWLR RMSE:\", lwlr_rmse)\n",
    "print(\"LWLR MAE:\", lwlr_mae)\n",
    "print(\"LWLR R-squared:\", lwlr_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e88c2",
   "metadata": {},
   "source": [
    "Based on the evaluation metrics provided, here are the 12 best models, ranked by RMSE, MAE, and R-squared values:\n",
    "\n",
    "1. Random Forest Regressor (RMSE: 4.5129170688300314e-15, MAE: 3.731747590232183e-15, R-squared: 1.0)\n",
    "2. lwlr\n",
    "3. Poly Kernel Regression (RMSE: 1.6127342607400202e-14, MAE: 1.2373097451168137e-14, R-squared: 1.0)\n",
    "4. RBF Kernel Regression (RMSE: 8.402295816463806e-12, MAE: 7.30005059367716e-12, R-squared: 1.0)\n",
    "5. Gaussian Process Regression (RMSE: 1.3658267901830649e-11, MAE: 1.321696512977017e-11, R-squared: 1.0)\n",
    "6. Weighted K-Nearest Neighbors (RMSE: 1.6151202582352662e-16, MAE: 6.094448088633645e-17, R-squared: 1.0)\n",
    "7. K-Nearest Neighbors (RMSE: 1.6151202582352662e-16, MAE: 6.094448088633645e-17, R-squared: 1.0)\n",
    "8. XGBoost Regressor (RMSE: 0.0001604180170547465, MAE: 0.00012571969879465955, R-squared: 0.9999999418520767)\n",
    "9. CatBoost Regressor (RMSE: 0.0004191192864975924, MAE: 0.00033127405418021457, R-squared: 0.9999996030797887)\n",
    "10. LightGBM Regressor (RMSE: 0.0032090330540582293, MAE: 0.002630020414763086, R-squared: 0.9999767310759211)\n",
    "11. Gradient Boosting Regressor (RMSE: 0.2065918821936626, MAE: 0.16683381944876388, R-squared: 0.9035606162870705)\n",
    "12. Ridge Regression (RMSE: 0.525636725173341, MAE: 0.429759090595957, R-squared: 0.37569139148812536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275b610",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5598987",
   "metadata": {},
   "source": [
    "## Random Forest Regressor HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Initialize Random Forest model\n",
    "random_forest = RandomForestRegressor()\n",
    "\n",
    "# Define the hyperparameters and their possible values for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # You can adjust these values\n",
    "    'max_depth': [None, 5, 10],      # You can adjust these values\n",
    "    'min_samples_split': [2, 5, 10] # You can adjust these values\n",
    "}\n",
    "\n",
    "# Initialize Grid Search for Random Forest\n",
    "grid_search_rf = GridSearchCV(random_forest, param_grid_rf, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Perform Grid Search for Random Forest\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Random Forest\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "\n",
    "# Predict on validation set using Random Forest\n",
    "rf_pred_val = best_rf.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Random Forest on validation set\n",
    "rf_rmse_val = mean_squared_error(y_val, rf_pred_val, squared=False)\n",
    "rf_mae_val = mean_absolute_error(y_val, rf_pred_val)\n",
    "rf_r2_val = r2_score(y_val, rf_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Random Forest on validation set\n",
    "print(\"Best Hyperparameters for Random Forest:\", best_params_rf)\n",
    "print(\"Random Forest RMSE (Validation):\", rf_rmse_val)\n",
    "print(\"Random Forest MAE (Validation):\", rf_mae_val)\n",
    "print(\"Random Forest R-squared (Validation):\", rf_r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e54dfc",
   "metadata": {},
   "source": [
    "## 2. LWLR HPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bbb10",
   "metadata": {},
   "source": [
    "# lwlr doesn't need hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbcb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the range of neighbors to consider\n",
    "param_grid_lwlr = {\n",
    "    'n_neighbors': [3, 5, 7],  # You can adjust these values\n",
    "    'weights': ['uniform', 'distance']  # You can adjust these values\n",
    "}\n",
    "\n",
    "# Initialize LWLR model\n",
    "lwlr = KNeighborsRegressor()\n",
    "\n",
    "# Initialize Grid Search for LWLR\n",
    "grid_search_lwlr = GridSearchCV(lwlr, param_grid_lwlr, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Perform Grid Search for LWLR\n",
    "grid_search_lwlr.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for LWLR\n",
    "best_lwlr = grid_search_lwlr.best_estimator_\n",
    "best_params_lwlr = grid_search_lwlr.best_params_\n",
    "\n",
    "# Predict on validation set using LWLR\n",
    "lwlr_pred_val = best_lwlr.predict(X_val)\n",
    "\n",
    "# Calculate metrics for LWLR on validation set\n",
    "lwlr_rmse_val = mean_squared_error(y_val, lwlr_pred_val, squared=False)\n",
    "lwlr_mae_val = mean_absolute_error(y_val, lwlr_pred_val)\n",
    "lwlr_r2_val = r2_score(y_val, lwlr_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for LWLR on validation set\n",
    "print(\"Best Hyperparameters for LWLR:\", best_params_lwlr)\n",
    "print(\"LWLR RMSE (Validation):\", lwlr_rmse_val)\n",
    "print(\"LWLR MAE (Validation):\", lwlr_mae_val)\n",
    "print(\"LWLR R-squared (Validation):\", lwlr_r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8da85",
   "metadata": {},
   "source": [
    "## 3. Poly Kernel Regression  HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_dist = {\n",
    "    'alpha': Real(1e-3, 1e+3, prior='log-uniform'),\n",
    "    'degree': Integer(2, 5)\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization for Poly Kernel Regression\n",
    "bayes_search_poly_kernel = BayesSearchCV(\n",
    "    KernelRidge(kernel='poly'),\n",
    "    param_dist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_iter=10,  # Reduced number of iterations\n",
    "    random_state=42,  # Set a seed for reproducibility\n",
    "    n_jobs=-1,  # Use multiple cores for parallelization\n",
    "    verbose=1,  # Print progress\n",
    "    n_points=5,  # Number of points to sample in each iteration\n",
    "    refit=True,  # Refit the best estimator with the entire dataset\n",
    ")\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search_poly_kernel.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Poly Kernel Regression\n",
    "best_poly_kernel = bayes_search_poly_kernel.best_estimator_\n",
    "best_params_poly_kernel = bayes_search_poly_kernel.best_params_\n",
    "\n",
    "# Predict on validation set using Poly Kernel Regression\n",
    "poly_kernel_pred_val = best_poly_kernel.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Poly Kernel Regression on validation set\n",
    "poly_kernel_rmse_val = mean_squared_error(y_val, poly_kernel_pred_val, squared=False)\n",
    "poly_kernel_mae_val = mean_absolute_error(y_val, poly_kernel_pred_val)\n",
    "poly_kernel_r2_val = r2_score(y_val, poly_kernel_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Poly Kernel Regression on validation set\n",
    "print(\"Best Hyperparameters for Poly Kernel Regression:\", best_params_poly_kernel)\n",
    "print(\"Poly Kernel RMSE (Validation):\", poly_kernel_rmse_val)\n",
    "print(\"Poly Kernel MAE (Validation):\", poly_kernel_mae_val)\n",
    "print(\"Poly Kernel R-squared (Validation):\", poly_kernel_r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a38ec",
   "metadata": {},
   "source": [
    "## 4. RBF Kernel Regression HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_dist = {\n",
    "    'alpha': Real(1e-3, 1e+3, prior='log-uniform'),\n",
    "    'gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "    'solver': Categorical(['auto', 'cholesky', 'lsqr', 'sparse_cg', 'dense'])\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization for RBF Kernel Regression\n",
    "bayes_search_rbf_kernel = BayesSearchCV(\n",
    "    KernelRidge(kernel='rbf'),\n",
    "    param_dist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_iter=10,  # Reduced number of iterations\n",
    "    random_state=42,  # Set a seed for reproducibility\n",
    "    n_jobs=-1,  # Use multiple cores for parallelization\n",
    "    verbose=1,  # Print progress\n",
    "    n_points=5,  # Number of points to sample in each iteration\n",
    "    n_restarts_optimizer=2,  # Number of restarts for the optimizer\n",
    "    refit=True  # Refit the best estimator with the entire dataset\n",
    ")\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search_rbf_kernel.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for RBF Kernel Regression\n",
    "best_rbf_kernel = bayes_search_rbf_kernel.best_estimator_\n",
    "best_params_rbf_kernel = bayes_search_rbf_kernel.best_params_\n",
    "\n",
    "# Predict on validation set using RBF Kernel Regression\n",
    "rbf_kernel_pred_val = best_rbf_kernel.predict(X_val)\n",
    "\n",
    "# Calculate metrics for RBF Kernel Regression on validation set\n",
    "rbf_kernel_rmse_val = mean_squared_error(y_val, rbf_kernel_pred_val, squared=False)\n",
    "rbf_kernel_mae_val = mean_absolute_error(y_val, rbf_kernel_pred_val)\n",
    "rbf_kernel_r2_val = r2_score(y_val, rbf_kernel_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for RBF Kernel Regression on validation set\n",
    "print(\"Best Hyperparameters for RBF Kernel Regression:\", best_params_rbf_kernel)\n",
    "print(\"RBF Kernel RMSE (Validation):\", rbf_kernel_rmse_val)\n",
    "print(\"RBF Kernel MAE (Validation):\", rbf_kernel_mae_val)\n",
    "print(\"RBF Kernel R-squared (Validation):\", rbf_kernel_r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117bf8f",
   "metadata": {},
   "source": [
    "## 5. Gaussian Process Regression HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for Gaussian Process Regression\n",
    "param_grid_gpr = {\n",
    "    'kernel': [None, 1.0 * RBF(length_scale=1.0), Matern(length_scale=1.0, nu=1.5), WhiteKernel(noise_level=1.0)],\n",
    "    'optimizer': ['fmin_l_bfgs_b', 'fmin_tnc', 'fmin_cobyla'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Initialize Grid Search for Gaussian Process Regression\n",
    "grid_search_gpr = GridSearchCV(GaussianProcessRegressor(), param_grid_gpr, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_gpr.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Gaussian Process Regression\n",
    "best_gpr = grid_search_gpr.best_estimator_\n",
    "best_params_gpr = grid_search_gpr.best_params_\n",
    "\n",
    "# Predict on validation set using Gaussian Process Regression\n",
    "gpr_pred_val = best_gpr.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Gaussian Process Regression on validation set\n",
    "gpr_rmse_val = mean_squared_error(y_val, gpr_pred_val, squared=False)\n",
    "gpr_mae_val = mean_absolute_error(y_val, gpr_pred_val)\n",
    "gpr_r2_val = r2_score(y_val, gpr_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Gaussian Process Regression on validation set\n",
    "print(\"Best Hyperparameters for Gaussian Process Regression:\", best_params_gpr)\n",
    "print(\"Gaussian Process RMSE (Validation):\", gpr_rmse_val)\n",
    "print(\"Gaussian Process MAE (Validation):\", gpr_mae_val)\n",
    "print(\"Gaussian Process R-squared (Validation):\", gpr_r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1973dc",
   "metadata": {},
   "source": [
    "## 6. Weighted K-Nearest Neighbors HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d4b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for Weighted K-Nearest Neighbors\n",
    "param_grid_wknn = {\n",
    "     'n_neighbors': [3, 5, 7, 9,11],  # Adjust as needed\n",
    "    'weights': ['uniform', 'distance'],  # These are the most common, but you can explore other weighting options if available\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm used to compute the nearest neighbors\n",
    "    'leaf_size': [10, 30, 50],  # Leaf size for tree-based algorithms\n",
    "    'p': [1, 2],  # Power parameter for the Minkowski metric\n",
    "    'metric': ['euclidean', 'manhattan']  #\n",
    "}\n",
    "\n",
    "# Initialize Grid Search for Weighted K-Nearest Neighbors\n",
    "grid_search_wknn = GridSearchCV(KNeighborsRegressor(), param_grid_wknn, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_wknn.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Weighted K-Nearest Neighbors\n",
    "best_wknn = grid_search_wknn.best_estimator_\n",
    "best_params_wknn = grid_search_wknn.best_params_\n",
    "\n",
    "# Predict on validation set using Weighted K-Nearest Neighbors\n",
    "wknn_pred_val = best_wknn.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Weighted K-Nearest Neighbors on validation set\n",
    "wknn_rmse_val = mean_squared_error(y_val, wknn_pred_val, squared=False)\n",
    "wknn_mae_val = mean_absolute_error(y_val, wknn_pred_val)\n",
    "wknn_r2_val = r2_score(y_val, wknn_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Weighted K-Nearest Neighbors on validation set\n",
    "print(\"Best Hyperparameters for Weighted K-Nearest Neighbors:\", best_params_wknn)\n",
    "print(\"Weighted K-NN RMSE (Validation):\", wknn_rmse_val)\n",
    "print(\"Weighted K-NN MAE (Validation):\", wknn_mae_val)\n",
    "print(\"Weighted K-NN R-squared (Validation):\", wknn_r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804b6f2",
   "metadata": {},
   "source": [
    "## 7. K-Nearest Neighbors HPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942d0aa",
   "metadata": {},
   "source": [
    "### The KNeighborsRegressor (KNN) model doesn't have traditional hyperparameters like other models (e.g., Random Forest).However, I performed hyperparameter tuning for the Locally Weighted Linear Regression (LWLR) using Grid Search with a specified range of neighbors and weight options, ultimately finding the best hyperparameters and model for LWLR, and evaluating its performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cfb6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the range of neighbors and weights to consider\n",
    "param_grid = {\n",
    "     'n_neighbors': [3, 5, 7, 9,11],  # Adjust as needed\n",
    "    'weights': ['uniform', 'distance'],  # These are the most common, but you can explore other weighting options if available\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm used to compute the nearest neighbors\n",
    "    'leaf_size': [10, 30, 50],  # Leaf size for tree-based algorithms\n",
    "    'p': [1, 2],  # Power parameter for the Minkowski metric\n",
    "    'metric': ['euclidean', 'manhattan'] \n",
    "}\n",
    "\n",
    "# Initialize KNN model\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Initialize Grid Search\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the KNN model with the best hyperparameters\n",
    "best_knn = KNeighborsRegressor(n_neighbors=best_params['n_neighbors'], weights=best_params['weights'])\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "knn_pred = best_knn.predict(X_val)\n",
    "\n",
    "# Calculate metrics for KNN\n",
    "knn_rmse = mean_squared_error(y_val, knn_pred, squared=False)\n",
    "knn_mae = mean_absolute_error(y_val, knn_pred)\n",
    "knn_r2 = r2_score(y_val, knn_pred)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for KNN\n",
    "print(\"Best Hyperparameters for KNN:\", best_params)\n",
    "print(\"KNN RMSE:\", knn_rmse)\n",
    "print(\"KNN MAE:\", knn_mae)\n",
    "print(\"KNN R-squared:\", knn_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923e3f8",
   "metadata": {},
   "source": [
    "## 8. XGBoost Regressor HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdca4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for XGBoost Regressor\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],  \n",
    "    'max_depth': [3, 4, 5],      \n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Initialize Grid Search for XGBoost Regressor\n",
    "grid_search_xgb = GridSearchCV(XGBRegressor(), param_grid_xgb, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for XGBoost Regressor\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "\n",
    "# Predict on validation set using XGBoost Regressor\n",
    "xgb_pred_val = best_xgb.predict(X_val)\n",
    "\n",
    "# Calculate metrics for XGBoost Regressor on validation set\n",
    "xgb_rmse_val = mean_squared_error(y_val, xgb_pred_val, squared=False)\n",
    "xgb_mae_val = mean_absolute_error(y_val, xgb_pred_val)\n",
    "xgb_r2_val = r2_score(y_val, xgb_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for XGBoost Regressor on validation set\n",
    "print(\"Best Hyperparameters for XGBoost Regressor:\", best_params_xgb)\n",
    "print(\"XGBoost RMSE (Validation):\", xgb_rmse_val)\n",
    "print(\"XGBoost MAE (Validation):\", xgb_mae_val)\n",
    "print(\"XGBoost R-squared (Validation):\", xgb_r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d242ccc",
   "metadata": {},
   "source": [
    "## 9. CatBoost Regressor  HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for CatBoost Regressor\n",
    "param_grid_catboost = {\n",
    "    'iterations': [100, 200, 300],  \n",
    "    'depth': [4, 6, 8],      \n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "     'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize Grid Search for CatBoost Regressor\n",
    "grid_search_catboost = GridSearchCV(CatBoostRegressor(verbose=0), param_grid_catboost, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_catboost.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for CatBoost Regressor\n",
    "best_catboost = grid_search_catboost.best_estimator_\n",
    "best_params_catboost = grid_search_catboost.best_params_\n",
    "\n",
    "# Predict on validation set using CatBoost Regressor\n",
    "catboost_pred_val = best_catboost.predict(X_val)\n",
    "\n",
    "# Calculate metrics for CatBoost Regressor on validation set\n",
    "catboost_rmse_val = mean_squared_error(y_val, catboost_pred_val, squared=False)\n",
    "catboost_mae_val = mean_absolute_error(y_val, catboost_pred_val)\n",
    "catboost_r2_val = r2_score(y_val, catboost_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for CatBoost Regressor on validation set\n",
    "print(\"Best Hyperparameters for CatBoost Regressor:\", best_params_catboost)\n",
    "print(\"CatBoost RMSE (Validation):\", catboost_rmse_val)\n",
    "print(\"CatBoost MAE (Validation):\", catboost_mae_val)\n",
    "print(\"CatBoost R-squared (Validation):\", catboost_r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4551bd",
   "metadata": {},
   "source": [
    "## 10. LightGBM Regressor HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for LightGBM Regressor\n",
    "param_grid_lgb = {\n",
    "    'n_estimators': [100, 200, 300],  \n",
    "    'max_depth': [3, 4, 5],      \n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize Grid Search for LightGBM Regressor\n",
    "grid_search_lgb = GridSearchCV(LGBMRegressor(), param_grid_lgb, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for LightGBM Regressor\n",
    "best_lgb = grid_search_lgb.best_estimator_\n",
    "best_params_lgb = grid_search_lgb.best_params_\n",
    "\n",
    "# Predict on validation set using LightGBM Regressor\n",
    "lgb_pred_val = best_lgb.predict(X_val)\n",
    "\n",
    "# Calculate metrics for LightGBM Regressor on validation set\n",
    "lgb_rmse_val = mean_squared_error(y_val, lgb_pred_val, squared=False)\n",
    "lgb_mae_val = mean_absolute_error(y_val, lgb_pred_val)\n",
    "lgb_r2_val = r2_score(y_val, lgb_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for LightGBM Regressor on validation set\n",
    "print(\"Best Hyperparameters for LightGBM Regressor:\", best_params_lgb)\n",
    "print(\"LightGBM RMSE (Validation):\", lgb_rmse_val)\n",
    "print(\"LightGBM MAE (Validation):\", lgb_mae_val)\n",
    "print(\"LightGBM R-squared (Validation):\", lgb_r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc2443",
   "metadata": {},
   "source": [
    "## 11. Gradient Boosting Regressor HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84785a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for Gradient Boosting Regressor\n",
    "param_grid_gbr = {\n",
    "    'n_estimators': [100, 200, 300],  \n",
    "    'max_depth': [3, 4, 5],      \n",
    "    'learning_rate': [0.01, 0.1, 0.2]  \n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize Grid Search for Gradient Boosting Regressor\n",
    "grid_search_gbr = GridSearchCV(GradientBoostingRegressor(), param_grid_gbr, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_gbr.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Gradient Boosting Regressor\n",
    "best_gbr = grid_search_gbr.best_estimator_\n",
    "best_params_gbr = grid_search_gbr.best_params_\n",
    "\n",
    "# Predict on validation set using Gradient Boosting Regressor\n",
    "gbr_pred_val = best_gbr.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Gradient Boosting Regressor on validation set\n",
    "gbr_rmse_val = mean_squared_error(y_val, gbr_pred_val, squared=False)\n",
    "gbr_mae_val = mean_absolute_error(y_val, gbr_pred_val)\n",
    "gbr_r2_val = r2_score(y_val, gbr_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Gradient Boosting Regressor on validation set\n",
    "print(\"Best Hyperparameters for Gradient Boosting Regressor:\", best_params_gbr)\n",
    "print(\"Gradient Boosting RMSE (Validation):\", gbr_rmse_val)\n",
    "print(\"Gradient Boosting MAE (Validation):\", gbr_mae_val)\n",
    "print(\"Gradient Boosting R-squared (Validation):\", gbr_r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44b872",
   "metadata": {},
   "source": [
    "## 12.Ridge Regression HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for Ridge Regression\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']} \n",
    "\n",
    "# Initialize Ridge model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Initialize Grid Search\n",
    "grid_search_ridge = GridSearchCV(ridge, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Train the Grid Search\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and model for Ridge Regression\n",
    "best_ridge = grid_search_ridge.best_estimator_\n",
    "best_params_ridge = grid_search_ridge.best_params_\n",
    "\n",
    "# Predict on validation set using Ridge Regression\n",
    "ridge_pred = best_ridge.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Ridge Regression\n",
    "ridge_rmse = mean_squared_error(y_val, ridge_pred, squared=False)\n",
    "ridge_mae = mean_absolute_error(y_val, ridge_pred)\n",
    "ridge_r2 = r2_score(y_val, ridge_pred)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Ridge Regression\n",
    "print(\"Best Hyperparameters for Ridge Regression:\", best_params_ridge)\n",
    "print(\"Ridge Regression RMSE:\", ridge_rmse)\n",
    "print(\"Ridge Regression MAE:\", ridge_mae)\n",
    "print(\"Ridge Regression R-squared:\", ridge_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d10bd2",
   "metadata": {},
   "source": [
    "# Hybrid models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already tuned and evaluated the models as you provided\n",
    "\n",
    "# Define the weighted average ensemble\n",
    "def weighted_average_ensemble(models, weights, X):\n",
    "    predictions = np.column_stack([model.predict(X) for model in models])\n",
    "    weighted_predictions = np.sum(predictions * weights, axis=1) / np.sum(weights)\n",
    "    return weighted_predictions\n",
    "\n",
    "# Define the models and their corresponding weights for the ensemble\n",
    "models = [best_rf, best_lwlr, best_gpr, best_wknn, best_xgb, best_catboost, best_lgb, best_gbr, best_ridge]\n",
    "weights = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Adjust weights as needed\n",
    "\n",
    "# Predict on validation set using the ensemble\n",
    "ensemble_pred_val = weighted_average_ensemble(models, weights, X_val)\n",
    "\n",
    "# Calculate metrics for the ensemble on validation set\n",
    "ensemble_rmse_val = mean_squared_error(y_val, ensemble_pred_val, squared=False)\n",
    "ensemble_mae_val = mean_absolute_error(y_val, ensemble_pred_val)\n",
    "ensemble_r2_val = r2_score(y_val, ensemble_pred_val)\n",
    "\n",
    "# Print evaluation metrics for the ensemble\n",
    "print(\"Ensemble RMSE (Validation):\", ensemble_rmse_val)\n",
    "print(\"Ensemble MAE (Validation):\", ensemble_mae_val)\n",
    "print(\"Ensemble R-squared (Validation):\", ensemble_r2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weights for each model\n",
    "weight_rf = 0.2\n",
    "weight_lwlr = 0.2\n",
    "weight_poly_kernel = 0.1\n",
    "weight_rbf_kernel = 0.1\n",
    "weight_gpr = 0.1\n",
    "weight_wknn = 0.1\n",
    "weight_knn = 0.1\n",
    "weight_xgb = 0.05\n",
    "weight_catboost = 0.05\n",
    "weight_lgb = 0.05\n",
    "weight_gbr = 0.05\n",
    "weight_ridge = 0.05\n",
    "\n",
    "# Create hybrid predictions\n",
    "hybrid_pred_val = (\n",
    "    weight_rf * rf_pred_val +\n",
    "    weight_lwlr * lwlr_pred_val +\n",
    "    weight_poly_kernel * poly_kernel_pred_val +\n",
    "    weight_rbf_kernel * rbf_kernel_pred_val +\n",
    "    weight_gpr * gpr_pred_val +\n",
    "    weight_wknn * wknn_pred_val +\n",
    "    weight_knn * knn_pred_val +\n",
    "    weight_xgb * xgb_pred_val +\n",
    "    weight_catboost * catboost_pred_val +\n",
    "    weight_lgb * lgb_pred_val +\n",
    "    weight_gbr * gbr_pred_val +\n",
    "    weight_ridge * ridge_pred_val\n",
    ")\n",
    "\n",
    "# Calculate metrics for the hybrid model on validation set\n",
    "hybrid_rmse_val = mean_squared_error(y_val, hybrid_pred_val, squared=False)\n",
    "hybrid_mae_val = mean_absolute_error(y_val, hybrid_pred_val)\n",
    "hybrid_r2_val = r2_score(y_val, hybrid_pred_val)\n",
    "\n",
    "# Print evaluation metrics for the hybrid model on validation set\n",
    "print(\"Hybrid RMSE (Validation):\", hybrid_rmse_val)\n",
    "print(\"Hybrid MAE (Validation):\", hybrid_mae_val)\n",
    "print(\"Hybrid R-squared (Validation):\", hybrid_r2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for krr_pred and ridge_pred\n",
    "hybrid1_pred = (weight_krr * krr_pred + weight_ridge * ridge_pred)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 1\n",
    "hybrid1_rmse = mean_squared_error(y_val, hybrid1_pred, squared=False)\n",
    "hybrid1_mae = mean_absolute_error(y_val, hybrid1_pred)\n",
    "hybrid1_r2 = r2_score(y_val, hybrid1_pred)\n",
    "\n",
    "print(\"Hybrid Model 1 (KRR + Ridge) RMSE:\", hybrid1_rmse)\n",
    "print(\"Hybrid Model 1 (KRR + Ridge) MAE:\", hybrid1_mae)\n",
    "print(\"Hybrid Model 1 (KRR + Ridge) R-squared:\", hybrid1_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf03f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for lwlr_pred_val and wknn_pred_val\n",
    "lwlr_wknn_pred_val = lwlr_pred_val * wknn_pred_val\n",
    "\n",
    "# Assuming you have predictions for best_krr\n",
    "hybrid2_pred_val = best_krr.predict(X_val) + lwlr_wknn_pred_val\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 2\n",
    "hybrid2_rmse_val = mean_squared_error(y_val, hybrid2_pred_val, squared=False)\n",
    "hybrid2_mae_val = mean_absolute_error(y_val, hybrid2_pred_val)\n",
    "hybrid2_r2_val = r2_score(y_val, hybrid2_pred_val)\n",
    "\n",
    "print(\"Hybrid Model 2 (KRR + (LWLR * Weighted K-NN)) RMSE:\", hybrid2_rmse_val)\n",
    "print(\"Hybrid Model 2 (KRR + (LWLR * Weighted K-NN)) MAE:\", hybrid2_mae_val)\n",
    "print(\"Hybrid Model 2 (KRR + (LWLR * Weighted K-NN)) R-squared:\", hybrid2_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f025295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for krr_pred, lwlr_pred_val, and wknn_pred_val\n",
    "hybrid3_pred_val = krr_pred + lwlr_pred_val + wknn_pred_val\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 3\n",
    "hybrid3_rmse_val = mean_squared_error(y_val, hybrid3_pred_val, squared=False)\n",
    "hybrid3_mae_val = mean_absolute_error(y_val, hybrid3_pred_val)\n",
    "hybrid3_r2_val = r2_score(y_val, hybrid3_pred_val)\n",
    "\n",
    "print(\"Hybrid Model 3 (KRR (Ensemble) + LWLR + Weighted K-NN) RMSE:\", hybrid3_rmse_val)\n",
    "print(\"Hybrid Model 3 (KRR (Ensemble) + LWLR + Weighted K-NN) MAE:\", hybrid3_mae_val)\n",
    "print(\"Hybrid Model 3 (KRR (Ensemble) + LWLR + Weighted K-NN) R-squared:\", hybrid3_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65660afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for krr_pred and lwlr_pred_val\n",
    "hybrid4_pred_val = (krr_pred + lwlr_pred_val) / 2\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 4\n",
    "hybrid4_rmse_val = mean_squared_error(y_val, hybrid4_pred_val, squared=False)\n",
    "hybrid4_mae_val = mean_absolute_error(y_val, hybrid4_pred_val)\n",
    "hybrid4_r2_val = r2_score(y_val, hybrid4_pred_val)\n",
    "\n",
    "print(\"Hybrid Model 4 (KRR + LWLR) RMSE:\", hybrid4_rmse_val)\n",
    "print(\"Hybrid Model 4 (KRR + LWLR) MAE:\", hybrid4_mae_val)\n",
    "print(\"Hybrid Model 4 (KRR + LWLR) R-squared:\", hybrid4_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db86f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for krr_pred, lwlr_pred_val, and wknn_pred_val\n",
    "hybrid5_pred_val = (krr_pred + lwlr_pred_val + wknn_pred_val) / 3\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 5\n",
    "hybrid5_rmse_val = mean_squared_error(y_val, hybrid5_pred_val, squared=False)\n",
    "hybrid5_mae_val = mean_absolute_error(y_val, hybrid5_pred_val)\n",
    "hybrid5_r2_val = r2_score(y_val, hybrid5_pred_val)\n",
    "\n",
    "print(\"Hybrid Model 5 (KRR + LWLR + Weighted K-NN) RMSE:\", hybrid5_rmse_val)\n",
    "print(\"Hybrid Model 5 (KRR + LWLR + Weighted K-NN) MAE:\", hybrid5_mae_val)\n",
    "print(\"Hybrid Model 5 (KRR + LWLR + Weighted K-NN) R-squared:\", hybrid5_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de04a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for rf_pred_val and lwlr_pred_val\n",
    "hybrid6_pred_val = (rf_pred_val + lwlr_pred_val) / 2\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 6\n",
    "hybrid6_rmse_val = mean_squared_error(y_val, hybrid6_pred_val, squared=False)\n",
    "hybrid6_mae_val = mean_absolute_error(y_val, hybrid6_pred_val)\n",
    "hybrid6_r2_val = r2_score(y_val, hybrid6_pred_val)\n",
    "\n",
    "print(\"Hybrid Model 6 (Random Forest + LWLR) RMSE:\", hybrid6_rmse_val)\n",
    "print(\"Hybrid Model 6 (Random Forest + LWLR) MAE:\", hybrid6_mae_val)\n",
    "print(\"Hybrid Model 6 (Random Forest + LWLR) R-squared:\", hybrid6_r2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for rf_pred_val and lwlr_pred_val\n",
    "hybrid1_pred = (weight_rf * rf_pred_val + weight_lwlr * lwlr_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 1 on the validation set\n",
    "hybrid1_rmse = mean_squared_error(y_val, hybrid1_pred, squared=False)\n",
    "hybrid1_mae = mean_absolute_error(y_val, hybrid1_pred)\n",
    "hybrid1_r2 = r2_score(y_val, hybrid1_pred)\n",
    "\n",
    "print(\"Hybrid Model 1 (RF + LWLR) RMSE:\", hybrid1_rmse)\n",
    "print(\"Hybrid Model 1 (RF + LWLR) MAE:\", hybrid1_mae)\n",
    "print(\"Hybrid Model 1 (RF + LWLR) R-squared:\", hybrid1_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for poly_kernel_pred_val and rbf_kernel_pred_val\n",
    "hybrid2_pred = (weight_poly_kernel * poly_kernel_pred_val + weight_rbf_kernel * rbf_kernel_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 2 on the validation set\n",
    "hybrid2_rmse = mean_squared_error(y_val, hybrid2_pred, squared=False)\n",
    "hybrid2_mae = mean_absolute_error(y_val, hybrid2_pred)\n",
    "hybrid2_r2 = r2_score(y_val, hybrid2_pred)\n",
    "\n",
    "print(\"Hybrid Model 2 (KRR (Poly Kernel) + RBF Kernel) RMSE:\", hybrid2_rmse)\n",
    "print(\"Hybrid Model 2 (KRR (Poly Kernel) + RBF Kernel) MAE:\", hybrid2_mae)\n",
    "print(\"Hybrid Model 2 (KRR (Poly Kernel) + RBF Kernel) R-squared:\", hybrid2_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33556f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for xgb_pred_val and catboost_pred_val\n",
    "hybrid3_pred = (weight_xgb * xgb_pred_val + weight_catboost * catboost_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 3 on the validation set\n",
    "hybrid3_rmse = mean_squared_error(y_val, hybrid3_pred, squared=False)\n",
    "hybrid3_mae = mean_absolute_error(y_val, hybrid3_pred)\n",
    "hybrid3_r2 = r2_score(y_val, hybrid3_pred)\n",
    "\n",
    "print(\"Hybrid Model 3 (XGBoost + CatBoost) RMSE:\", hybrid3_rmse)\n",
    "print(\"Hybrid Model 3 (XGBoost + CatBoost) MAE:\", hybrid3_mae)\n",
    "print(\"Hybrid Model 3 (XGBoost + CatBoost) R-squared:\", hybrid3_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for lgb_pred_val and gbr_pred_val\n",
    "hybrid4_pred = (weight_lgb * lgb_pred_val + weight_gbr * gbr_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 4 on the validation set\n",
    "hybrid4_rmse = mean_squared_error(y_val, hybrid4_pred, squared=False)\n",
    "hybrid4_mae = mean_absolute_error(y_val, hybrid4_pred)\n",
    "hybrid4_r2 = r2_score(y_val, hybrid4_pred)\n",
    "\n",
    "print(\"Hybrid Model 4 (LightGBM + Gradient Boosting) RMSE:\", hybrid4_rmse)\n",
    "print(\"Hybrid Model 4 (LightGBM + Gradient Boosting) MAE:\", hybrid4_mae)\n",
    "print(\"Hybrid Model 4 (LightGBM + Gradient Boosting) R-squared:\", hybrid4_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for rf_pred_val, lwlr_pred_val, and xgb_pred_val\n",
    "hybrid6_pred = (weight_rf * rf_pred_val + weight_lwlr * lwlr_pred_val + weight_xgb * xgb_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 6 on the validation set\n",
    "hybrid6_rmse = mean_squared_error(y_val, hybrid6_pred, squared=False)\n",
    "hybrid6_mae = mean_absolute_error(y_val, hybrid6_pred)\n",
    "hybrid6_r2 = r2_score(y_val, hybrid6_pred)\n",
    "\n",
    "print(\"Hybrid Model 6 (RF + LWLR + XGBoost) RMSE:\", hybrid6_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have rf_pred_val and lwlr_pred_val\n",
    "hybrid1_pred = (weight_rf * rf_pred_val + weight_lwlr * lwlr_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 1\n",
    "hybrid1_rmse = mean_squared_error(y_val, hybrid1_pred, squared=False)\n",
    "hybrid1_mae = mean_absolute_error(y_val, hybrid1_pred)\n",
    "hybrid1_r2 = r2_score(y_val, hybrid1_pred)\n",
    "\n",
    "print(\"Hybrid Model 1 (RF + LWLR) RMSE:\", hybrid1_rmse)\n",
    "print(\"Hybrid Model 1 (RF + LWLR) MAE:\", hybrid1_mae)\n",
    "print(\"Hybrid Model 1 (RF + LWLR) R-squared:\", hybrid1_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66727a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have lgb_pred_val and catboost_pred_val\n",
    "hybrid3_pred = (weight_lgb * lgb_pred_val + weight_catboost * catboost_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 3\n",
    "hybrid3_rmse = mean_squared_error(y_val, hybrid3_pred, squared=False)\n",
    "hybrid3_mae = mean_absolute_error(y_val, hybrid3_pred)\n",
    "hybrid3_r2 = r2_score(y_val, hybrid3_pred)\n",
    "\n",
    "print(\"Hybrid Model 3 (LightGBM + CatBoost) RMSE:\", hybrid3_rmse)\n",
    "print(\"Hybrid Model 3 (LightGBM + CatBoost) MAE:\", hybrid3_mae)\n",
    "print(\"Hybrid Model 3 (LightGBM + CatBoost) R-squared:\", hybrid3_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have ridge_pred and poly_kernel_pred_val\n",
    "hybrid5_pred = (weight_ridge * ridge_pred + weight_poly_kernel * poly_kernel_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 5\n",
    "hybrid5_rmse = mean_squared_error(y_val, hybrid5_pred, squared=False)\n",
    "hybrid5_mae = mean_absolute_error(y_val, hybrid5_pred)\n",
    "hybrid5_r2 = r2_score(y_val, hybrid5_pred)\n",
    "\n",
    "print(\"Hybrid Model 5 (Ridge + Poly Kernel) RMSE:\", hybrid5_rmse)\n",
    "print(\"Hybrid Model 5 (Ridge + Poly Kernel) MAE:\", hybrid5_mae)\n",
    "print(\"Hybrid Model 5 (Ridge + Poly Kernel) R-squared:\", hybrid5_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have rf_pred_val, lgb_pred_val, and catboost_pred_val\n",
    "\n",
    "# Define weights for models (you can adjust these)\n",
    "weight_rf = 0.4\n",
    "weight_lgb = 0.3\n",
    "weight_catboost = 0.3\n",
    "\n",
    "# Combine predictions\n",
    "hybrid7_pred = (weight_rf * rf_pred_val + weight_lgb * lgb_pred_val + weight_catboost * catboost_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 7\n",
    "hybrid7_rmse = mean_squared_error(y_val, hybrid7_pred, squared=False)\n",
    "hybrid7_mae = mean_absolute_error(y_val, hybrid7_pred)\n",
    "hybrid7_r2 = r2_score(y_val, hybrid7_pred)\n",
    "\n",
    "print(\"Hybrid Model 7 (RF + LightGBM + CatBoost) RMSE:\", hybrid7_rmse)\n",
    "print(\"Hybrid Model 7 (RF + LightGBM + CatBoost) MAE:\", hybrid7_mae)\n",
    "print(\"Hybrid Model 7 (RF + LightGBM + CatBoost) R-squared:\", hybrid7_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878fe0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming you have xgb_pred_val, gbr_pred_val, and lgb_pred_val\n",
    "\n",
    "# Initialize the Meta-model (you can choose a different one if desired)\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "# Create the training set for the meta-model\n",
    "meta_X_train = np.column_stack((xgb_pred_val, gbr_pred_val, lgb_pred_val))\n",
    "\n",
    "# Train the Meta-model\n",
    "meta_model.fit(meta_X_train, y_val)\n",
    "\n",
    "# Create the test set for the meta-model\n",
    "meta_X_val = np.column_stack((xgb_pred_val, gbr_pred_val, lgb_pred_val))\n",
    "\n",
    "# Predict using the Meta-model\n",
    "hybrid8_pred = meta_model.predict(meta_X_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 8\n",
    "hybrid8_rmse = mean_squared_error(y_val, hybrid8_pred, squared=False)\n",
    "hybrid8_mae = mean_absolute_error(y_val, hybrid8_pred)\n",
    "hybrid8_r2 = r2_score(y_val, hybrid8_pred)\n",
    "\n",
    "print(\"Hybrid Model 8 (Stacking Ensemble) RMSE:\", hybrid8_rmse)\n",
    "print(\"Hybrid Model 8 (Stacking Ensemble) MAE:\", hybrid8_mae)\n",
    "print(\"Hybrid Model 8 (Stacking Ensemble) R-squared:\", hybrid8_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68151c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have predictions from all your tuned models stored in variables\n",
    "# rf_pred_val, lwlr_pred_val, poly_kernel_pred_val, rbf_kernel_pred_val, gpr_pred_val, wknn_pred_val, knn_pred_val, xgb_pred_val, catboost_pred_val, lgb_pred_val, gbr_pred_val, ridge_pred_val\n",
    "\n",
    "# Define weights for each model (you can adjust these)\n",
    "weights = {\n",
    "    'rf': 0.1,\n",
    "    'lwlr': 0.1,\n",
    "    'poly_kernel': 0.1,\n",
    "    'rbf_kernel': 0.1,\n",
    "    'gpr': 0.1,\n",
    "    'wknn': 0.1,\n",
    "    'knn': 0.1,\n",
    "    'xgb': 0.1,\n",
    "    'catboost': 0.1,\n",
    "    'lgb': 0.1,\n",
    "    'gbr': 0.1,\n",
    "    'ridge': 0.1\n",
    "}\n",
    "\n",
    "# List of model predictions\n",
    "predictions = [\n",
    "    rf_pred_val, lwlr_pred_val, poly_kernel_pred_val, rbf_kernel_pred_val,\n",
    "    gpr_pred_val, wknn_pred_val, knn_pred_val, xgb_pred_val, catboost_pred_val,\n",
    "    lgb_pred_val, gbr_pred_val, ridge_pred_val\n",
    "]\n",
    "\n",
    "# Initialize an array to store the weighted predictions\n",
    "weighted_predictions = np.zeros_like(rf_pred_val)\n",
    "\n",
    "# Combine predictions with weights\n",
    "for model_pred, weight in zip(predictions, weights.values()):\n",
    "    weighted_predictions += model_pred * weight\n",
    "\n",
    "# Calculate metrics for the weighted predictions\n",
    "weighted_rmse_val = mean_squared_error(y_val, weighted_predictions, squared=False)\n",
    "weighted_mae_val = mean_absolute_error(y_val, weighted_predictions)\n",
    "weighted_r2_val = r2_score(y_val, weighted_predictions)\n",
    "\n",
    "# Print the evaluation metrics for the weighted predictions\n",
    "print(\"Weighted Average RMSE (Validation):\", weighted_rmse_val)\n",
    "print(\"Weighted Average MAE (Validation):\", weighted_mae_val)\n",
    "print(\"Weighted Average R-squared (Validation):\", weighted_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_test, y_test, and all tuned models are defined\n",
    "\n",
    "# Get predictions for all tuned models on the test set\n",
    "rf_test_pred = best_rf.predict(X_test)\n",
    "lwlr_test_pred = best_lwlr.predict(X_test)\n",
    "poly_kernel_test_pred = best_poly_kernel.predict(X_test)\n",
    "rbf_kernel_test_pred = best_rbf_kernel.predict(X_test)\n",
    "gpr_test_pred = best_gpr.predict(X_test)\n",
    "wknn_test_pred = best_wknn.predict(X_test)\n",
    "knn_test_pred = best_knn.predict(X_test)\n",
    "xgb_test_pred = best_xgb.predict(X_test)\n",
    "catboost_test_pred = best_catboost.predict(X_test)\n",
    "lgb_test_pred = best_lgb.predict(X_test)\n",
    "gbr_test_pred = best_gbr.predict(X_test)\n",
    "ridge_test_pred = best_ridge.predict(X_test)\n",
    "\n",
    "# Define weights for each model's predictions\n",
    "weights = {\n",
    "    'rf': 0.05,\n",
    "    'lwlr': 0.05,\n",
    "    'poly_kernel': 0.1,\n",
    "    'rbf_kernel': 0.1,\n",
    "    'gpr': 0.05,\n",
    "    'wknn': 0.05,\n",
    "    'knn': 0.05,\n",
    "    'xgb': 0.1,\n",
    "    'catboost': 0.1,\n",
    "    'lgb': 0.1,\n",
    "    'gbr': 0.1,\n",
    "    'ridge': 0.1\n",
    "}\n",
    "\n",
    "# Combine predictions using weighted averaging\n",
    "hybrid_complex_test_pred = (\n",
    "    weights['rf']*rf_test_pred + weights['lwlr']*lwlr_test_pred +\n",
    "    weights['poly_kernel']*poly_kernel_test_pred + weights['rbf_kernel']*rbf_kernel_test_pred +\n",
    "    weights['gpr']*gpr_test_pred + weights['wknn']*wknn_test_pred +\n",
    "    weights['knn']*knn_test_pred + weights['xgb']*xgb_test_pred +\n",
    "    weights['catboost']*catboost_test_pred + weights['lgb']*lgb_test_pred +\n",
    "    weights['gbr']*gbr_test_pred + weights['ridge']*ridge_test_pred\n",
    ")\n",
    "\n",
    "# Calculate evaluation metrics for the complex hybrid model\n",
    "hybrid_complex_test_rmse = mean_squared_error(y_test, hybrid_complex_test_pred, squared=False)\n",
    "hybrid_complex_test_mae = mean_absolute_error(y_test, hybrid_complex_test_pred)\n",
    "hybrid_complex_test_r2 = r2_score(y_test, hybrid_complex_test_pred)\n",
    "\n",
    "# Print the evaluation metrics for the complex hybrid model\n",
    "print(\"Complex Hybrid Model Test RMSE:\", hybrid_complex_test_rmse)\n",
    "print(\"Complex Hybrid Model Test MAE:\", hybrid_complex_test_mae)\n",
    "print(\"Complex Hybrid Model Test R-squared:\", hybrid_complex_test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53b82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions from Random Forest, Ridge, KNN, and ElasticNet\n",
    "rf_pred = best_rf.predict(X_val)\n",
    "ridge_pred = best_ridge.predict(X_val)\n",
    "knn_pred = best_knn.predict(X_val)\n",
    "enet_pred = best_enet.predict(X_val)\n",
    "\n",
    "# Define weights for Random Forest, Ridge, KNN, and ElasticNet predictions\n",
    "weight_rf = 0.3  # Adjust as needed\n",
    "weight_ridge = 0.2\n",
    "weight_knn = 0.2\n",
    "weight_enet = 0.3\n",
    "\n",
    "hybrid5_pred = (weight_rf * rf_pred + weight_ridge * ridge_pred + weight_knn * knn_pred + weight_enet * enet_pred)\n",
    "\n",
    "\n",
    "hybrid5_rmse = mean_squared_error(y_val, hybrid5_pred, squared=False)\n",
    "hybrid5_mae = mean_absolute_error(y_val, hybrid5_pred)\n",
    "hybrid5_r2 = r2_score(y_val, hybrid5_pred)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Hybrid Model 5 (Random Forest + Ridge + KNN + ElasticNet) RMSE:\", hybrid5_rmse)\n",
    "print(\"Hybrid Model 5 (Random Forest + Ridge + KNN + ElasticNet) MAE:\", hybrid5_mae)\n",
    "print(\"Hybrid Model 5 (Random Forest + Ridge + KNN ) R-squared:\", hybrid5_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f5cd6",
   "metadata": {},
   "source": [
    "# Testing nusing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df.loc[(df['Date'].dt.year >= 1993) & (df['Date'].dt.year <= 2012)].drop(columns=['Rangpur-27 Satgora Mistripara (Rangpur Sadar)', 'Date']).to_numpy()\n",
    "y_test = df.loc[(df['Date'].dt.year >= 1993) & (df['Date'].dt.year <= 2012)]['Rangpur-27 Satgora Mistripara (Rangpur Sadar)'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bc332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d770df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e39b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_val is the actual target values and hybrid1_pred is the predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_val, hybrid1_pred, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values (Validation Set)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# For the test set\n",
    "# Assuming y_test is the actual target values and hybrid1_test_pred is the predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, hybrid1_test_pred, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values (Test Set)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df063dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
