{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6d594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51831ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rangpur_Tave</th>\n",
       "      <th>Rangpur_PRCP</th>\n",
       "      <th>Rangpur_NDVI</th>\n",
       "      <th>IOD_Value</th>\n",
       "      <th>SOI_Value</th>\n",
       "      <th>Nina3.4_Value</th>\n",
       "      <th>MEI_Value</th>\n",
       "      <th>Rangpur-27 Satgora Mistripara (Rangpur Sadar)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.1993</td>\n",
       "      <td>15.1</td>\n",
       "      <td>49</td>\n",
       "      <td>0.1763</td>\n",
       "      <td>-0.025962</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.925</td>\n",
       "      <td>3.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02.01.1993</td>\n",
       "      <td>15.1</td>\n",
       "      <td>49</td>\n",
       "      <td>0.1763</td>\n",
       "      <td>-0.025962</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.925</td>\n",
       "      <td>3.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03.01.1993</td>\n",
       "      <td>15.1</td>\n",
       "      <td>49</td>\n",
       "      <td>0.1763</td>\n",
       "      <td>-0.025962</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.925</td>\n",
       "      <td>3.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04.01.1993</td>\n",
       "      <td>15.1</td>\n",
       "      <td>49</td>\n",
       "      <td>0.1763</td>\n",
       "      <td>-0.025962</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.925</td>\n",
       "      <td>3.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05.01.1993</td>\n",
       "      <td>15.1</td>\n",
       "      <td>49</td>\n",
       "      <td>0.1763</td>\n",
       "      <td>-0.025962</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.925</td>\n",
       "      <td>3.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>27.12.2017</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>-0.204937</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>2.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9127</th>\n",
       "      <td>28.12.2017</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>-0.204937</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>2.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9128</th>\n",
       "      <td>29.12.2017</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>-0.204937</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>2.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>30.12.2017</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>-0.204937</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>2.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9130</th>\n",
       "      <td>31.12.2017</td>\n",
       "      <td>22.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>-0.204937</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>2.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9131 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Rangpur_Tave  Rangpur_PRCP  Rangpur_NDVI  IOD_Value  \\\n",
       "0     01.01.1993          15.1            49        0.1763  -0.025962   \n",
       "1     02.01.1993          15.1            49        0.1763  -0.025962   \n",
       "2     03.01.1993          15.1            49        0.1763  -0.025962   \n",
       "3     04.01.1993          15.1            49        0.1763  -0.025962   \n",
       "4     05.01.1993          15.1            49        0.1763  -0.025962   \n",
       "...          ...           ...           ...           ...        ...   \n",
       "9126  27.12.2017          22.6             0        0.1372  -0.204937   \n",
       "9127  28.12.2017          22.6             0        0.1372  -0.204937   \n",
       "9128  29.12.2017          22.6             0        0.1372  -0.204937   \n",
       "9129  30.12.2017          22.6             0        0.1372  -0.204937   \n",
       "9130  31.12.2017          22.6             0        0.1372  -0.204937   \n",
       "\n",
       "      SOI_Value  Nina3.4_Value  MEI_Value  \\\n",
       "0          -8.2           0.28      0.925   \n",
       "1          -8.2           0.28      0.925   \n",
       "2          -8.2           0.28      0.925   \n",
       "3          -8.2           0.28      0.925   \n",
       "4          -8.2           0.28      0.925   \n",
       "...         ...            ...        ...   \n",
       "9126       -1.4          -0.85     -0.404   \n",
       "9127       -1.4          -0.85     -0.404   \n",
       "9128       -1.4          -0.85     -0.404   \n",
       "9129       -1.4          -0.85     -0.404   \n",
       "9130       -1.4          -0.85     -0.404   \n",
       "\n",
       "      Rangpur-27 Satgora Mistripara (Rangpur Sadar)  \n",
       "0                                             3.286  \n",
       "1                                             3.286  \n",
       "2                                             3.286  \n",
       "3                                             3.286  \n",
       "4                                             3.286  \n",
       "...                                             ...  \n",
       "9126                                          2.800  \n",
       "9127                                          2.800  \n",
       "9128                                          2.800  \n",
       "9129                                          2.800  \n",
       "9130                                          2.800  \n",
       "\n",
       "[9131 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r'D:\\Jupyter\\Ground water level prediction(Towfiq Sir)\\final_data_updated.xlsx')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da445da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors in DataFrame:\n",
      "No errors found in DataFrame\n"
     ]
    }
   ],
   "source": [
    "from DataFrame_Checker import DataFrameChecker\n",
    "\n",
    "#  an instance of DataFrameChecker\n",
    "checker = DataFrameChecker(df)\n",
    "\n",
    "\n",
    "# Called the checking functions\n",
    "checker.check_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c36a8df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in DataFrame\n"
     ]
    }
   ],
   "source": [
    "checker.check_missing_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf276e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Rangpur_Tave', 'Rangpur_PRCP', 'Rangpur_NDVI', 'IOD_Value',\n",
      "       'SOI_Value', 'Nina3.4_Value', 'MEI_Value',\n",
      "       'Rangpur-27 Satgora Mistripara (Rangpur Sadar)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1878bf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9131 entries, 0 to 9130\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                         Non-Null Count  Dtype  \n",
      "---  ------                                         --------------  -----  \n",
      " 0   Date                                           9131 non-null   object \n",
      " 1   Rangpur_Tave                                   9131 non-null   float64\n",
      " 2   Rangpur_PRCP                                   9131 non-null   int64  \n",
      " 3   Rangpur_NDVI                                   9131 non-null   float64\n",
      " 4   IOD_Value                                      9131 non-null   float64\n",
      " 5   SOI_Value                                      9131 non-null   float64\n",
      " 6   Nina3.4_Value                                  9131 non-null   float64\n",
      " 7   MEI_Value                                      9131 non-null   float64\n",
      " 8   Rangpur-27 Satgora Mistripara (Rangpur Sadar)  9131 non-null   float64\n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 642.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c6b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "cols_to_convert = df.columns.difference(['Date'])\n",
    "\n",
    "df[cols_to_convert] = df[cols_to_convert].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8b1a0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9131.000000\n",
       "mean        3.089596\n",
       "std         0.660229\n",
       "min         1.212500\n",
       "25%         2.647500\n",
       "50%         3.125000\n",
       "75%         3.550000\n",
       "max         4.750000\n",
       "Name: Rangpur-27 Satgora Mistripara (Rangpur Sadar), dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Rangpur-27 Satgora Mistripara (Rangpur Sadar)\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fafc53f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9131 entries, 0 to 9130\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                         Non-Null Count  Dtype  \n",
      "---  ------                                         --------------  -----  \n",
      " 0   Date                                           9131 non-null   object \n",
      " 1   Rangpur_Tave                                   9131 non-null   float64\n",
      " 2   Rangpur_PRCP                                   9131 non-null   float64\n",
      " 3   Rangpur_NDVI                                   9131 non-null   float64\n",
      " 4   IOD_Value                                      9131 non-null   float64\n",
      " 5   SOI_Value                                      9131 non-null   float64\n",
      " 6   Nina3.4_Value                                  9131 non-null   float64\n",
      " 7   MEI_Value                                      9131 non-null   float64\n",
      " 8   Rangpur-27 Satgora Mistripara (Rangpur Sadar)  9131 non-null   float64\n",
      "dtypes: float64(8), object(1)\n",
      "memory usage: 642.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76beffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d.%m.%Y')\n",
    "\n",
    "df_train = df.loc[df['Date'].dt.year <= 2012]\n",
    "df_test = df.loc[df['Date'].dt.year >= 2013]\n",
    "\n",
    "X_train = df_train.drop(columns=['Rangpur-27 Satgora Mistripara (Rangpur Sadar)', 'Date']).to_numpy()\n",
    "y_train = df_train['Rangpur-27 Satgora Mistripara (Rangpur Sadar)'].to_numpy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f086d309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1289\n",
      "[LightGBM] [Info] Number of data points in the train set: 5844, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 3.094913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1289\n",
      "[LightGBM] [Info] Number of data points in the train set: 5844, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 3.094913\n",
      "Linear Regression RMSE: 0.5807501854776889\n",
      "Decision Tree RMSE: 1.0094950828885286e-15\n",
      "Random Forest RMSE: 4.518092466281155e-15\n",
      "Ridge Regression RMSE: 0.5807580200006073\n",
      "K-Nearest Neighbors RMSE: 1.6151202582352662e-16\n",
      "Gaussian Process RMSE: 1.3301686933559528e-11\n",
      "Polynomial Regression RMSE: 0.4055127033934677\n",
      "Poly Kernel RMSE: 0.405512702448706\n",
      "RBF Kernel RMSE: 1.2549158456250204e-11\n",
      "Gaussian Process Regression RMSE: 1.3301686933559528e-11\n",
      "Weighted K-NN RMSE: 1.6151202582352662e-16\n",
      "Gradient Boosting Regressor RMSE: 0.2535028161847791\n",
      "AdaBoost Regressor RMSE: 0.4390947742946434\n",
      "LightGBM Regressor RMSE: 0.006883847971166662\n",
      "CatBoost Regressor RMSE: 0.0014210994782213001\n",
      "XGBoost RMSE: 0.00019926970598050484\n",
      "LightGBM Regressor RMSE: 0.006883847971166662\n",
      "\n",
      "Linear Regression MAE: 0.4584230760351297\n",
      "Decision Tree MAE: 7.781439953567147e-16\n",
      "Random Forest MAE: 3.738890708939559e-15\n",
      "Ridge Regression MAE: 0.458384916528452\n",
      "K-Nearest Neighbors MAE: 6.094448088633645e-17\n",
      "Gaussian Process MAE: 1.2774273387505519e-11\n",
      "Polynomial Regression MAE: 0.3113893378447838\n",
      "Poly Kernel MAE: 0.3113893320696301\n",
      "RBF Kernel MAE: 8.969593491402283e-12\n",
      "Gaussian Process Regression MAE: 1.2774273387505519e-11\n",
      "Weighted K-NN MAE: 6.094448088633645e-17\n",
      "Gradient Boosting Regressor MAE: 0.1932137417962033\n",
      "AdaBoost Regressor MAE: 0.3708639272087772\n",
      "LightGBM Regressor MAE: 0.005509232341982605\n",
      "CatBoost Regressor MAE: 0.0011204719414542725\n",
      "XGBoost MAE: 0.00015809938313850414\n",
      "LightGBM Regressor MAE: 0.005509232341982605\n",
      "\n",
      "Linear Regression R-squared: 0.23790934538841502\n",
      "Decision Tree R-squared: 1.0\n",
      "Random Forest R-squared: 1.0\n",
      "Ridge Regression R-squared: 0.2378887835113066\n",
      "K-Nearest Neighbors R-squared: 1.0\n",
      "Gaussian Process R-squared: 1.0\n",
      "Polynomial Regression R-squared: 0.6284332453758397\n",
      "Poly Kernel R-squared: 0.6284332471071887\n",
      "RBF Kernel R-squared: 1.0\n",
      "Gaussian Process Regression R-squared: 1.0\n",
      "Weighted K-NN R-squared: 1.0\n",
      "Gradient Boosting Regressor R-squared: 0.854791012156572\n",
      "AdaBoost Regressor R-squared: 0.5643432385339491\n",
      "LightGBM Regressor R-squared: 0.9998929244133552\n",
      "CatBoost Regressor R-squared: 0.9999954367224799\n",
      "XGBoost R-squared: 0.9999999102756167\n",
      "LightGBM Regressor R-squared: 0.9998929244133552\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from tbats import TBATS\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "\n",
    "\n",
    "random_state = 42 \n",
    "\n",
    "# Initialize different base models\n",
    "linear_regression = LinearRegression()\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "random_forest = RandomForestRegressor(random_state=random_state)\n",
    "ridge = Ridge()\n",
    "knn = KNeighborsRegressor()\n",
    "gaussian_process = GaussianProcessRegressor()\n",
    "poly_reg = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
    "poly_kernel = make_pipeline(StandardScaler(), PolynomialFeatures(3), LinearRegression())\n",
    "rbf_kernel = make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=None, n_restarts_optimizer=10, random_state=random_state))\n",
    "gpr = GaussianProcessRegressor(random_state=random_state)\n",
    "weighted_knn = KNeighborsRegressor(weights='distance')\n",
    "lightgbm = LGBMRegressor(random_state=random_state)\n",
    "catboost = CatBoostRegressor(random_state=random_state, verbose=0)\n",
    "xgb_model = XGBRegressor()\n",
    "lgb_regressor = lgb.LGBMRegressor()\n",
    "gbr = GradientBoostingRegressor(random_state=random_state)\n",
    "abr = AdaBoostRegressor(random_state=random_state)\n",
    "\n",
    "\n",
    "# Train each base model on the training set\n",
    "linear_regression.fit(X_train, y_train)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "random_forest.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "gaussian_process.fit(X_train, y_train)\n",
    "poly_reg.fit(X_train, y_train)\n",
    "poly_kernel.fit(X_train, y_train)\n",
    "rbf_kernel.fit(X_train, y_train)\n",
    "gpr.fit(X_train, y_train)\n",
    "weighted_knn.fit(X_train, y_train)\n",
    "lightgbm.fit(X_train, y_train)\n",
    "catboost.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "lgb_regressor.fit(X_train, y_train)\n",
    "gbr.fit(X_train, y_train)\n",
    "abr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Train and evaluate Linear Regression\n",
    "linear_pred_val = linear_regression.predict(X_val)\n",
    "linear_rmse_val = mean_squared_error(y_val, linear_pred_val, squared=False)\n",
    "linear_mae_val = mean_absolute_error(y_val, linear_pred_val)\n",
    "linear_r2_val = r2_score(y_val, linear_pred_val)\n",
    "\n",
    "# Train and evaluate Decision Tree\n",
    "dt_pred_val = decision_tree.predict(X_val)\n",
    "dt_rmse_val = mean_squared_error(y_val, dt_pred_val, squared=False)\n",
    "dt_mae_val = mean_absolute_error(y_val, dt_pred_val)\n",
    "dt_r2_val = r2_score(y_val, dt_pred_val)\n",
    "\n",
    "# Train and evaluate Random Forest\n",
    "rf_pred_val = random_forest.predict(X_val)\n",
    "rf_rmse_val = mean_squared_error(y_val, rf_pred_val, squared=False)\n",
    "rf_mae_val = mean_absolute_error(y_val, rf_pred_val)\n",
    "rf_r2_val = r2_score(y_val, rf_pred_val)\n",
    "\n",
    "# Train and evaluate Ridge Regression\n",
    "ridge_pred_val = ridge.predict(X_val)\n",
    "ridge_rmse_val = mean_squared_error(y_val, ridge_pred_val, squared=False)\n",
    "ridge_mae_val = mean_absolute_error(y_val, ridge_pred_val)\n",
    "ridge_r2_val = r2_score(y_val, ridge_pred_val)\n",
    "\n",
    "\n",
    "# Train and evaluate K-Nearest Neighbors\n",
    "knn_pred_val = knn.predict(X_val)\n",
    "knn_rmse_val = mean_squared_error(y_val, knn_pred_val, squared=False)\n",
    "knn_mae_val = mean_absolute_error(y_val, knn_pred_val)\n",
    "knn_r2_val = r2_score(y_val, knn_pred_val)\n",
    "\n",
    "# Train and evaluate Gaussian Process\n",
    "gp_pred_val = gaussian_process.predict(X_val)\n",
    "gp_rmse_val = mean_squared_error(y_val, gp_pred_val, squared=False)\n",
    "gp_mae_val = mean_absolute_error(y_val, gp_pred_val)\n",
    "gp_r2_val = r2_score(y_val, gp_pred_val)\n",
    "\n",
    "\n",
    "# Train and evaluate Polynomial Regression\n",
    "poly_reg_pred_val = poly_reg.predict(X_val)\n",
    "poly_reg_rmse_val = mean_squared_error(y_val, poly_reg_pred_val, squared=False)\n",
    "poly_reg_mae_val = mean_absolute_error(y_val, poly_reg_pred_val)\n",
    "poly_reg_r2_val = r2_score(y_val, poly_reg_pred_val)\n",
    "\n",
    "# Train and evaluate Poly Kernel\n",
    "poly_kernel_pred_val = poly_kernel.predict(X_val)\n",
    "poly_kernel_rmse_val = mean_squared_error(y_val, poly_kernel_pred_val, squared=False)\n",
    "poly_kernel_mae_val = mean_absolute_error(y_val, poly_kernel_pred_val)\n",
    "poly_kernel_r2_val = r2_score(y_val, poly_kernel_pred_val)\n",
    "\n",
    "# Train and evaluate RBF Kernel\n",
    "rbf_kernel_pred_val = rbf_kernel.predict(X_val)\n",
    "rbf_kernel_rmse_val = mean_squared_error(y_val, rbf_kernel_pred_val, squared=False)\n",
    "rbf_kernel_mae_val = mean_absolute_error(y_val, rbf_kernel_pred_val)\n",
    "rbf_kernel_r2_val = r2_score(y_val, rbf_kernel_pred_val)\n",
    "\n",
    "# Train and evaluate Gaussian Process Regression\n",
    "gpr_pred_val = gpr.predict(X_val)\n",
    "gpr_rmse_val = mean_squared_error(y_val, gpr_pred_val, squared=False)\n",
    "gpr_mae_val = mean_absolute_error(y_val, gpr_pred_val)\n",
    "gpr_r2_val = r2_score(y_val, gpr_pred_val)\n",
    "\n",
    "# Train and evaluate Weighted K-NN\n",
    "wknn_pred_val = weighted_knn.predict(X_val)\n",
    "wknn_rmse_val = mean_squared_error(y_val, wknn_pred_val, squared=False)\n",
    "wknn_mae_val = mean_absolute_error(y_val, wknn_pred_val)\n",
    "wknn_r2_val = r2_score(y_val, wknn_pred_val)\n",
    "\n",
    "# Train and evaluate Gradient Boosting Regressor\n",
    "gbr_pred_val = gbr.predict(X_val)\n",
    "gbr_rmse_val = mean_squared_error(y_val, gbr_pred_val, squared=False)\n",
    "gbr_mae_val = mean_absolute_error(y_val, gbr_pred_val)\n",
    "gbr_r2_val = r2_score(y_val, gbr_pred_val)\n",
    "\n",
    "# Train and evaluate AdaBoost Regressor\n",
    "abr_pred_val = abr.predict(X_val)\n",
    "abr_rmse_val = mean_squared_error(y_val, abr_pred_val, squared=False)\n",
    "abr_mae_val = mean_absolute_error(y_val, abr_pred_val)\n",
    "abr_r2_val = r2_score(y_val, abr_pred_val)\n",
    "\n",
    "# Train and evaluate LightGBM Regressor\n",
    "lightgbm_pred_val = lightgbm.predict(X_val)\n",
    "lightgbm_rmse_val = mean_squared_error(y_val, lightgbm_pred_val, squared=False)\n",
    "lightgbm_mae_val = mean_absolute_error(y_val, lightgbm_pred_val)\n",
    "lightgbm_r2_val = r2_score(y_val, lightgbm_pred_val)\n",
    "\n",
    "# Train and evaluate CatBoost Regressor\n",
    "catboost_pred_val = catboost.predict(X_val)\n",
    "catboost_rmse_val = mean_squared_error(y_val, catboost_pred_val, squared=False)\n",
    "catboost_mae_val = mean_absolute_error(y_val, catboost_pred_val)\n",
    "catboost_r2_val = r2_score(y_val, catboost_pred_val)\n",
    "\n",
    "# Train and evaluate XGBoost\n",
    "xgb_pred_val = xgb_model.predict(X_val)\n",
    "xgb_rmse_val = mean_squared_error(y_val, xgb_pred_val, squared=False)\n",
    "xgb_mae_val = mean_absolute_error(y_val, xgb_pred_val)\n",
    "xgb_r2_val = r2_score(y_val, xgb_pred_val)\n",
    "\n",
    "# Train and evaluate LightGBM Regressor\n",
    "lgb_pred_val = lgb_regressor.predict(X_val)\n",
    "lgb_rmse_val = mean_squared_error(y_val, lgb_pred_val, squared=False)\n",
    "lgb_mae_val = mean_absolute_error(y_val, lgb_pred_val)\n",
    "lgb_r2_val = r2_score(y_val, lgb_pred_val)\n",
    "\n",
    "\n",
    "# Print evaluation metrics for each model on the validation set\n",
    "\n",
    "# Print the evaluation metrics for each model\n",
    "print(\"Linear Regression RMSE:\", linear_rmse_val)\n",
    "print(\"Decision Tree RMSE:\", dt_rmse_val)\n",
    "print(\"Random Forest RMSE:\", rf_rmse_val)\n",
    "print(\"Ridge Regression RMSE:\", ridge_rmse_val)\n",
    "print(\"K-Nearest Neighbors RMSE:\", knn_rmse_val)\n",
    "print(\"Gaussian Process RMSE:\", gp_rmse_val)\n",
    "print(\"Polynomial Regression RMSE:\", poly_reg_rmse_val)\n",
    "print(\"Poly Kernel RMSE:\", poly_kernel_rmse_val)\n",
    "print(\"RBF Kernel RMSE:\", rbf_kernel_rmse_val)\n",
    "print(\"Gaussian Process Regression RMSE:\", gpr_rmse_val)\n",
    "print(\"Weighted K-NN RMSE:\", wknn_rmse_val)\n",
    "print(\"Gradient Boosting Regressor RMSE:\", gbr_rmse_val)\n",
    "print(\"AdaBoost Regressor RMSE:\", abr_rmse_val)\n",
    "print(\"LightGBM Regressor RMSE:\", lightgbm_rmse_val)\n",
    "print(\"CatBoost Regressor RMSE:\", catboost_rmse_val)\n",
    "print(\"XGBoost RMSE:\", xgb_rmse_val)\n",
    "print(\"LightGBM Regressor RMSE:\", lgb_rmse_val)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Linear Regression MAE:\", linear_mae_val)\n",
    "print(\"Decision Tree MAE:\", dt_mae_val)\n",
    "print(\"Random Forest MAE:\", rf_mae_val)\n",
    "print(\"Ridge Regression MAE:\", ridge_mae_val)\n",
    "print(\"K-Nearest Neighbors MAE:\", knn_mae_val)\n",
    "print(\"Gaussian Process MAE:\", gp_mae_val)\n",
    "print(\"Polynomial Regression MAE:\", poly_reg_mae_val)\n",
    "print(\"Poly Kernel MAE:\", poly_kernel_mae_val)\n",
    "print(\"RBF Kernel MAE:\", rbf_kernel_mae_val)\n",
    "print(\"Gaussian Process Regression MAE:\", gpr_mae_val)\n",
    "print(\"Weighted K-NN MAE:\", wknn_mae_val)\n",
    "print(\"Gradient Boosting Regressor MAE:\", gbr_mae_val)\n",
    "print(\"AdaBoost Regressor MAE:\", abr_mae_val)\n",
    "print(\"LightGBM Regressor MAE:\", lightgbm_mae_val)\n",
    "print(\"CatBoost Regressor MAE:\", catboost_mae_val)\n",
    "print(\"XGBoost MAE:\", xgb_mae_val)\n",
    "print(\"LightGBM Regressor MAE:\", lgb_mae_val)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Linear Regression R-squared:\", linear_r2_val)\n",
    "print(\"Decision Tree R-squared:\", dt_r2_val)\n",
    "print(\"Random Forest R-squared:\", rf_r2_val)\n",
    "print(\"Ridge Regression R-squared:\", ridge_r2_val)\n",
    "print(\"K-Nearest Neighbors R-squared:\", knn_r2_val)\n",
    "print(\"Gaussian Process R-squared:\", gp_r2_val)\n",
    "print(\"Polynomial Regression R-squared:\", poly_reg_r2_val)\n",
    "print(\"Poly Kernel R-squared:\", poly_kernel_r2_val)\n",
    "print(\"RBF Kernel R-squared:\", rbf_kernel_r2_val)\n",
    "print(\"Gaussian Process Regression R-squared:\", gpr_r2_val)\n",
    "print(\"Weighted K-NN R-squared:\", wknn_r2_val)\n",
    "print(\"Gradient Boosting Regressor R-squared:\", gbr_r2_val)\n",
    "print(\"AdaBoost Regressor R-squared:\", abr_r2_val)\n",
    "print(\"LightGBM Regressor R-squared:\", lightgbm_r2_val)\n",
    "print(\"CatBoost Regressor R-squared:\", catboost_r2_val)\n",
    "print(\"XGBoost R-squared:\", xgb_r2_val)\n",
    "print(\"LightGBM Regressor R-squared:\", lgb_r2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6607b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LWLR RMSE: 1.6151202582352662e-16\n",
      "LWLR MAE: 6.094448088633645e-17\n",
      "LWLR R-squared: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the range of neighbors to consider\n",
    "num_neighbors = 5  # You can adjust this value\n",
    "\n",
    "# Initialize LWLR model\n",
    "lwlr = KNeighborsRegressor(n_neighbors=num_neighbors, weights='distance')\n",
    "\n",
    "# Train the LWLR model\n",
    "lwlr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "lwlr_pred = lwlr.predict(X_val)\n",
    "\n",
    "# Calculate metrics for LWLR\n",
    "lwlr_rmse = mean_squared_error(y_val, lwlr_pred, squared=False)\n",
    "lwlr_mae = mean_absolute_error(y_val, lwlr_pred)\n",
    "lwlr_r2 = r2_score(y_val, lwlr_pred)\n",
    "\n",
    "# Print the evaluation metrics for LWLR\n",
    "print(\"LWLR RMSE:\", lwlr_rmse)\n",
    "print(\"LWLR MAE:\", lwlr_mae)\n",
    "print(\"LWLR R-squared:\", lwlr_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b825ef5",
   "metadata": {},
   "source": [
    "Based on the evaluation metrics provided, here are the 12 best models, ranked by RMSE, MAE, and R-squared values:\n",
    "\n",
    "1. Random Forest Regressor (RMSE: 4.5129170688300314e-15, MAE: 3.731747590232183e-15, R-squared: 1.0)\n",
    "2. lwlr\n",
    "3. Poly Kernel Regression (RMSE: 1.6127342607400202e-14, MAE: 1.2373097451168137e-14, R-squared: 1.0)\n",
    "4. RBF Kernel Regression (RMSE: 8.402295816463806e-12, MAE: 7.30005059367716e-12, R-squared: 1.0)\n",
    "5. Gaussian Process Regression (RMSE: 1.3658267901830649e-11, MAE: 1.321696512977017e-11, R-squared: 1.0)\n",
    "6. Weighted K-Nearest Neighbors (RMSE: 1.6151202582352662e-16, MAE: 6.094448088633645e-17, R-squared: 1.0)\n",
    "7. K-Nearest Neighbors (RMSE: 1.6151202582352662e-16, MAE: 6.094448088633645e-17, R-squared: 1.0)\n",
    "8. XGBoost Regressor (RMSE: 0.0001604180170547465, MAE: 0.00012571969879465955, R-squared: 0.9999999418520767)\n",
    "9. CatBoost Regressor (RMSE: 0.0004191192864975924, MAE: 0.00033127405418021457, R-squared: 0.9999996030797887)\n",
    "10. LightGBM Regressor (RMSE: 0.0032090330540582293, MAE: 0.002630020414763086, R-squared: 0.9999767310759211)\n",
    "11. Gradient Boosting Regressor (RMSE: 0.2065918821936626, MAE: 0.16683381944876388, R-squared: 0.9035606162870705)\n",
    "12. Ridge Regression (RMSE: 0.525636725173341, MAE: 0.429759090595957, R-squared: 0.37569139148812536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275b610",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f792678",
   "metadata": {},
   "source": [
    "## Random Forest Regressor HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db09e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Random Forest: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Random Forest RMSE (Validation): 2.1805342279387936e-15\n",
      "Random Forest MAE (Validation): 1.7725573081729229e-15\n",
      "Random Forest R-squared (Validation): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Initialize Random Forest model\n",
    "random_forest = RandomForestRegressor()\n",
    "\n",
    "# Define the hyperparameters and their possible values for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # You can adjust these values\n",
    "    'max_depth': [None, 5, 10],      # You can adjust these values\n",
    "    'min_samples_split': [2, 5, 10] # You can adjust these values\n",
    "}\n",
    "\n",
    "# Initialize Grid Search for Random Forest\n",
    "grid_search_rf = GridSearchCV(random_forest, param_grid_rf, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Perform Grid Search for Random Forest\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Random Forest\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "\n",
    "# Predict on validation set using Random Forest\n",
    "rf_pred_val = best_rf.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Random Forest on validation set\n",
    "rf_rmse_val = mean_squared_error(y_val, rf_pred_val, squared=False)\n",
    "rf_mae_val = mean_absolute_error(y_val, rf_pred_val)\n",
    "rf_r2_val = r2_score(y_val, rf_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Random Forest on validation set\n",
    "print(\"Best Hyperparameters for Random Forest:\", best_params_rf)\n",
    "print(\"Random Forest RMSE (Validation):\", rf_rmse_val)\n",
    "print(\"Random Forest MAE (Validation):\", rf_mae_val)\n",
    "print(\"Random Forest R-squared (Validation):\", rf_r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a1f75",
   "metadata": {},
   "source": [
    "## 2. LWLR HPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bbb10",
   "metadata": {},
   "source": [
    "# lwlr doesn't need hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94dbcb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for LWLR: {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "LWLR RMSE (Validation): 1.6151202582352662e-16\n",
      "LWLR MAE (Validation): 6.094448088633645e-17\n",
      "LWLR R-squared (Validation): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the range of neighbors to consider\n",
    "param_grid_lwlr = {\n",
    "    'n_neighbors': [3, 5, 7],  # You can adjust these values\n",
    "    'weights': ['uniform', 'distance']  # You can adjust these values\n",
    "}\n",
    "\n",
    "# Initialize LWLR model\n",
    "lwlr = KNeighborsRegressor()\n",
    "\n",
    "# Initialize Grid Search for LWLR\n",
    "grid_search_lwlr = GridSearchCV(lwlr, param_grid_lwlr, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Perform Grid Search for LWLR\n",
    "grid_search_lwlr.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for LWLR\n",
    "best_lwlr = grid_search_lwlr.best_estimator_\n",
    "best_params_lwlr = grid_search_lwlr.best_params_\n",
    "\n",
    "# Predict on validation set using LWLR\n",
    "lwlr_pred_val = best_lwlr.predict(X_val)\n",
    "\n",
    "# Calculate metrics for LWLR on validation set\n",
    "lwlr_rmse_val = mean_squared_error(y_val, lwlr_pred_val, squared=False)\n",
    "lwlr_mae_val = mean_absolute_error(y_val, lwlr_pred_val)\n",
    "lwlr_r2_val = r2_score(y_val, lwlr_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for LWLR on validation set\n",
    "print(\"Best Hyperparameters for LWLR:\", best_params_lwlr)\n",
    "print(\"LWLR RMSE (Validation):\", lwlr_rmse_val)\n",
    "print(\"LWLR MAE (Validation):\", lwlr_mae_val)\n",
    "print(\"LWLR R-squared (Validation):\", lwlr_r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3baa4a6",
   "metadata": {},
   "source": [
    "## 3. Poly Kernel Regression  HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for Poly Kernel Regression\n",
    "param_grid_poly_kernel = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'degree': [2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize Grid Search for Poly Kernel Regression\n",
    "grid_search_poly_kernel = GridSearchCV(\n",
    "    KernelRidge(kernel='poly'),\n",
    "    param_grid_poly_kernel,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_poly_kernel.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Poly Kernel Regression\n",
    "best_poly_kernel = grid_search_poly_kernel.best_estimator_\n",
    "best_params_poly_kernel = grid_search_poly_kernel.best_params_\n",
    "\n",
    "# Predict on validation set using Poly Kernel Regression\n",
    "poly_kernel_pred_val = best_poly_kernel.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Poly Kernel Regression on validation set\n",
    "poly_kernel_rmse_val = mean_squared_error(y_val, poly_kernel_pred_val, squared=False)\n",
    "poly_kernel_mae_val = mean_absolute_error(y_val, poly_kernel_pred_val)\n",
    "poly_kernel_r2_val = r2_score(y_val, poly_kernel_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Poly Kernel Regression on validation set\n",
    "print(\"Best Hyperparameters for Poly Kernel Regression:\", best_params_poly_kernel)\n",
    "print(\"Poly Kernel RMSE (Validation):\", poly_kernel_rmse_val)\n",
    "print(\"Poly Kernel MAE (Validation):\", poly_kernel_mae_val)\n",
    "print(\"Poly Kernel R-squared (Validation):\", poly_kernel_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f05535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_dist = {\n",
    "    'alpha': Real(1e-3, 1e+3, prior='log-uniform'),\n",
    "    'degree': Integer(2, 5)\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization for Poly Kernel Regression\n",
    "bayes_search_poly_kernel = BayesSearchCV(\n",
    "    KernelRidge(kernel='poly'),\n",
    "    param_dist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_iter=10,  # Adjust the number of iterations as needed\n",
    "    random_state=42,  # Set a seed for reproducibility\n",
    "    n_jobs=-1,  # Use multiple cores for parallelization\n",
    "    verbose=1,  # Print progress\n",
    "    n_points=5,  # Number of points to sample in each iteration\n",
    "    refit=True  # Refit the best estimator with the entire dataset\n",
    ")\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search_poly_kernel.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Poly Kernel Regression\n",
    "best_poly_kernel = bayes_search_poly_kernel.best_estimator_\n",
    "best_params_poly_kernel = bayes_search_poly_kernel.best_params_\n",
    "\n",
    "# Predict on validation set using Poly Kernel Regression\n",
    "poly_kernel_pred_val = best_poly_kernel.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Poly Kernel Regression on validation set\n",
    "poly_kernel_rmse_val = mean_squared_error(y_val, poly_kernel_pred_val, squared=False)\n",
    "poly_kernel_mae_val = mean_absolute_error(y_val, poly_kernel_pred_val)\n",
    "poly_kernel_r2_val = r2_score(y_val, poly_kernel_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Poly Kernel Regression on validation set\n",
    "print(\"Best Hyperparameters for Poly Kernel Regression:\", best_params_poly_kernel)\n",
    "print(\"Poly Kernel RMSE (Validation):\", poly_kernel_rmse_val)\n",
    "print(\"Poly Kernel MAE (Validation):\", poly_kernel_mae_val)\n",
    "print(\"Poly Kernel R-squared (Validation):\", poly_kernel_r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02eb9d1",
   "metadata": {},
   "source": [
    "## 4. RBF Kernel Regression HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for RBF Kernel Regression\n",
    "param_grid_rbf_kernel = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'gamma': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "    'solver': ['auto', 'cholesky', 'lsqr', 'sparse_cg', 'dense']\n",
    "}\n",
    "\n",
    "# Initialize Grid Search for RBF Kernel Regression\n",
    "grid_search_rbf_kernel = GridSearchCV(\n",
    "    KernelRidge(kernel='rbf'),\n",
    "    param_grid_rbf_kernel,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_rbf_kernel.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for RBF Kernel Regression\n",
    "best_rbf_kernel = grid_search_rbf_kernel.best_estimator_\n",
    "best_params_rbf_kernel = grid_search_rbf_kernel.best_params_\n",
    "\n",
    "# Predict on validation set using RBF Kernel Regression\n",
    "rbf_kernel_pred_val = best_rbf_kernel.predict(X_val)\n",
    "\n",
    "# Calculate metrics for RBF Kernel Regression on validation set\n",
    "rbf_kernel_rmse_val = mean_squared_error(y_val, rbf_kernel_pred_val, squared=False)\n",
    "rbf_kernel_mae_val = mean_absolute_error(y_val, rbf_kernel_pred_val)\n",
    "rbf_kernel_r2_val = r2_score(y_val, rbf_kernel_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for RBF Kernel Regression on validation set\n",
    "print(\"Best Hyperparameters for RBF Kernel Regression:\", best_params_rbf_kernel)\n",
    "print(\"RBF Kernel RMSE (Validation):\", rbf_kernel_rmse_val)\n",
    "print(\"RBF Kernel MAE (Validation):\", rbf_kernel_mae_val)\n",
    "print(\"RBF Kernel R-squared (Validation):\", rbf_kernel_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbaf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_dist = {\n",
    "    'alpha': Real(1e-3, 1e+3, prior='log-uniform'),\n",
    "    'solver': Categorical(['auto', 'cholesky', 'lsqr', 'sparse_cg', 'dense'])\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization for RBF Kernel Regression\n",
    "bayes_search_rbf_kernel = BayesSearchCV(\n",
    "    KernelRidge(kernel='rbf'),\n",
    "    param_dist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_iter=10,  # Adjust the number of iterations as needed\n",
    "    random_state=42,  # Set a seed for reproducibility\n",
    "    n_jobs=-1,  # Use multiple cores for parallelization\n",
    "    verbose=1,  # Print progress\n",
    "    n_points=5,  # Number of points to sample in each iteration\n",
    "    n_restarts_optimizer=2,  # Number of restarts for the optimizer\n",
    "    refit=True  # Refit the best estimator with the entire dataset\n",
    ")\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search_rbf_kernel.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for RBF Kernel Regression\n",
    "best_rbf_kernel = bayes_search_rbf_kernel.best_estimator_\n",
    "best_params_rbf_kernel = bayes_search_rbf_kernel.best_params_\n",
    "\n",
    "# Predict on validation set using RBF Kernel Regression\n",
    "rbf_kernel_pred_val = best_rbf_kernel.predict(X_val)\n",
    "\n",
    "# Calculate metrics for RBF Kernel Regression on validation set\n",
    "rbf_kernel_rmse_val = mean_squared_error(y_val, rbf_kernel_pred_val, squared=False)\n",
    "rbf_kernel_mae_val = mean_absolute_error(y_val, rbf_kernel_pred_val)\n",
    "rbf_kernel_r2_val = r2_score(y_val, rbf_kernel_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for RBF Kernel Regression on validation set\n",
    "print(\"Best Hyperparameters for RBF Kernel Regression:\", best_params_rbf_kernel)\n",
    "print(\"RBF Kernel RMSE (Validation):\", rbf_kernel_rmse_val)\n",
    "print(\"RBF Kernel MAE (Validation):\", rbf_kernel_mae_val)\n",
    "print(\"RBF Kernel R-squared (Validation):\", rbf_kernel_r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b7143",
   "metadata": {},
   "source": [
    "## 5. Gaussian Process Regression HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0f12d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Gaussian Process Regression: {'kernel': None}\n",
      "Gaussian Process RMSE (Validation): 1.3301686933559528e-11\n",
      "Gaussian Process MAE (Validation): 1.2774273387505519e-11\n",
      "Gaussian Process R-squared (Validation): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for Gaussian Process Regression\n",
    "param_grid_gpr = {\n",
    "    'kernel': [None, 1.0 * RBF(length_scale=1.0), Matern(length_scale=1.0, nu=1.5), WhiteKernel(noise_level=1.0)]   \n",
    "    \n",
    "}\n",
    "\n",
    "# Initialize Grid Search for Gaussian Process Regression\n",
    "grid_search_gpr = GridSearchCV(GaussianProcessRegressor(), param_grid_gpr, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_gpr.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Gaussian Process Regression\n",
    "best_gpr = grid_search_gpr.best_estimator_\n",
    "best_params_gpr = grid_search_gpr.best_params_\n",
    "\n",
    "# Predict on validation set using Gaussian Process Regression\n",
    "gpr_pred_val = best_gpr.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Gaussian Process Regression on validation set\n",
    "gpr_rmse_val = mean_squared_error(y_val, gpr_pred_val, squared=False)\n",
    "gpr_mae_val = mean_absolute_error(y_val, gpr_pred_val)\n",
    "gpr_r2_val = r2_score(y_val, gpr_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Gaussian Process Regression on validation set\n",
    "print(\"Best Hyperparameters for Gaussian Process Regression:\", best_params_gpr)\n",
    "print(\"Gaussian Process RMSE (Validation):\", gpr_rmse_val)\n",
    "print(\"Gaussian Process MAE (Validation):\", gpr_mae_val)\n",
    "print(\"Gaussian Process R-squared (Validation):\", gpr_r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8427dd2",
   "metadata": {},
   "source": [
    "## 6. Weighted K-Nearest Neighbors HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1df08573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Weighted K-Nearest Neighbors: {'algorithm': 'auto', 'leaf_size': 10, 'metric': 'euclidean', 'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}\n",
      "Weighted K-NN RMSE (Validation): 1.439459012139203e-16\n",
      "Weighted K-NN MAE (Validation): 5.1065699695284406e-17\n",
      "Weighted K-NN R-squared (Validation): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for Weighted K-Nearest Neighbors\n",
    "param_grid_wknn = {\n",
    "     'n_neighbors': [3, 5, 7, 9,11],  # Adjust as needed\n",
    "    'weights': ['uniform', 'distance'],  # These are the most common, but you can explore other weighting options if available\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm used to compute the nearest neighbors\n",
    "    'leaf_size': [10, 30, 50],  # Leaf size for tree-based algorithms\n",
    "    'p': [1, 2],  # Power parameter for the Minkowski metric\n",
    "    'metric': ['euclidean', 'manhattan']  #\n",
    "}\n",
    "\n",
    "# Initialize Grid Search for Weighted K-Nearest Neighbors\n",
    "grid_search_wknn = GridSearchCV(KNeighborsRegressor(), param_grid_wknn, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_wknn.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Weighted K-Nearest Neighbors\n",
    "best_wknn = grid_search_wknn.best_estimator_\n",
    "best_params_wknn = grid_search_wknn.best_params_\n",
    "\n",
    "# Predict on validation set using Weighted K-Nearest Neighbors\n",
    "wknn_pred_val = best_wknn.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Weighted K-Nearest Neighbors on validation set\n",
    "wknn_rmse_val = mean_squared_error(y_val, wknn_pred_val, squared=False)\n",
    "wknn_mae_val = mean_absolute_error(y_val, wknn_pred_val)\n",
    "wknn_r2_val = r2_score(y_val, wknn_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Weighted K-Nearest Neighbors on validation set\n",
    "print(\"Best Hyperparameters for Weighted K-Nearest Neighbors:\", best_params_wknn)\n",
    "print(\"Weighted K-NN RMSE (Validation):\", wknn_rmse_val)\n",
    "print(\"Weighted K-NN MAE (Validation):\", wknn_mae_val)print(\"Weighted K-NN R-squared (Validation):\", wknn_r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed47c40",
   "metadata": {},
   "source": [
    "## 7. K-Nearest Neighbors HPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942d0aa",
   "metadata": {},
   "source": [
    "### The KNeighborsRegressor (KNN) model doesn't have traditional hyperparameters like other models (e.g., Random Forest).However, I performed hyperparameter tuning for the Locally Weighted Linear Regression (LWLR) using Grid Search with a specified range of neighbors and weight options, ultimately finding the best hyperparameters and model for LWLR, and evaluating its performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71cfb6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for KNN: {'algorithm': 'auto', 'leaf_size': 10, 'metric': 'euclidean', 'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}\n",
      "KNN RMSE: 1.439459012139203e-16\n",
      "KNN MAE: 5.1065699695284406e-17\n",
      "KNN R-squared: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the range of neighbors and weights to consider\n",
    "param_grid = {\n",
    "     'n_neighbors': [3, 5, 7, 9,11],  # Adjust as needed\n",
    "    'weights': ['uniform', 'distance'],  # These are the most common, but you can explore other weighting options if available\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm used to compute the nearest neighbors\n",
    "    'leaf_size': [10, 30, 50],  # Leaf size for tree-based algorithms\n",
    "    'p': [1, 2],  # Power parameter for the Minkowski metric\n",
    "    'metric': ['euclidean', 'manhattan'] \n",
    "}\n",
    "\n",
    "# Initialize KNN model\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Initialize Grid Search\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the KNN model with the best hyperparameters\n",
    "best_knn = KNeighborsRegressor(n_neighbors=best_params['n_neighbors'], weights=best_params['weights'])\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "knn_pred = best_knn.predict(X_val)\n",
    "\n",
    "# Calculate metrics for KNN\n",
    "knn_rmse = mean_squared_error(y_val, knn_pred, squared=False)\n",
    "knn_mae = mean_absolute_error(y_val, knn_pred)\n",
    "knn_r2 = r2_score(y_val, knn_pred)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for KNN\n",
    "print(\"Best Hyperparameters for KNN:\", best_params)\n",
    "print(\"KNN RMSE:\", knn_rmse)\n",
    "print(\"KNN MAE:\", knn_mae)\n",
    "print(\"KNN R-squared:\", knn_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467add4",
   "metadata": {},
   "source": [
    "## 8. XGBoost Regressor HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b7d2924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Hyperparameters for XGBoost Regressor: OrderedDict([('gamma', 0.009563585051707605), ('learning_rate', 0.13856447107447098), ('max_depth', 5), ('n_estimators', 162)])\n",
      "XGBoost RMSE (Validation): 0.02703829621550143\n",
      "XGBoost MAE (Validation): 0.02076948565339519\n",
      "XGBoost R-squared (Validation): 0.998348089305399\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_dist = {\n",
    "    'n_estimators': Integer(100, 300),\n",
    "    'max_depth': Integer(3, 5),\n",
    "    'learning_rate': Real(0.01, 0.2, prior='uniform'),\n",
    "    'gamma': Real(0, 0.2, prior='uniform')\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization for XGBoost Regressor\n",
    "bayes_search_xgb = BayesSearchCV(\n",
    "    XGBRegressor(),\n",
    "    param_dist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_iter=10,  # Adjust the number of iterations as needed\n",
    "    random_state=42,  # Set a seed for reproducibility\n",
    "    n_jobs=-1,  # Use multiple cores for parallelization\n",
    "    verbose=1,  # Print progress\n",
    "    n_points=5,  # Number of points to sample in each iteration\n",
    "    refit=True  # Refit the best estimator with the entire dataset\n",
    ")\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for XGBoost Regressor\n",
    "best_xgb = bayes_search_xgb.best_estimator_\n",
    "best_params_xgb = bayes_search_xgb.best_params_\n",
    "\n",
    "# Predict on validation set using XGBoost Regressor\n",
    "xgb_pred_val = best_xgb.predict(X_val)\n",
    "\n",
    "# Calculate metrics for XGBoost Regressor on validation set\n",
    "xgb_rmse_val = mean_squared_error(y_val, xgb_pred_val, squared=False)\n",
    "xgb_mae_val = mean_absolute_error(y_val, xgb_pred_val)\n",
    "xgb_r2_val = r2_score(y_val, xgb_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for XGBoost Regressor on validation set\n",
    "print(\"Best Hyperparameters for XGBoost Regressor:\", best_params_xgb)\n",
    "print(\"XGBoost RMSE (Validation):\", xgb_rmse_val)\n",
    "print(\"XGBoost MAE (Validation):\", xgb_mae_val)\n",
    "print(\"XGBoost R-squared (Validation):\", xgb_r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0c450",
   "metadata": {},
   "source": [
    "## 9. CatBoost Regressor  HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b00d2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Hyperparameters for CatBoost Regressor: OrderedDict([('depth', 8), ('iterations', 226), ('learning_rate', 0.19853585997384401)])\n",
      "CatBoost RMSE (Validation): 7.982628541721926e-05\n",
      "CatBoost MAE (Validation): 6.421662678767478e-05\n",
      "CatBoost R-squared (Validation): 0.9999999856014167\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_dist = {\n",
    "    'iterations': Integer(100, 300),\n",
    "    'depth': Integer(4, 8),\n",
    "    'learning_rate': Real(0.01, 0.2, prior='uniform')\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization for CatBoost Regressor\n",
    "bayes_search_catboost = BayesSearchCV(\n",
    "    CatBoostRegressor(verbose=0),\n",
    "    param_dist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_iter=10,  # Adjust the number of iterations as needed\n",
    "    random_state=42,  # Set a seed for reproducibility\n",
    "    n_jobs=-1,  # Use multiple cores for parallelization\n",
    "    verbose=1,  # Print progress\n",
    "    n_points=5,  # Number of points to sample in each iteration\n",
    "    refit=True  # Refit the best estimator with the entire dataset\n",
    ")\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search_catboost.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for CatBoost Regressor\n",
    "best_catboost = bayes_search_catboost.best_estimator_\n",
    "best_params_catboost = bayes_search_catboost.best_params_\n",
    "\n",
    "# Predict on validation set using CatBoost Regressor\n",
    "catboost_pred_val = best_catboost.predict(X_val)\n",
    "\n",
    "# Calculate metrics for CatBoost Regressor on validation set\n",
    "catboost_rmse_val = mean_squared_error(y_val, catboost_pred_val, squared=False)\n",
    "catboost_mae_val = mean_absolute_error(y_val, catboost_pred_val)\n",
    "catboost_r2_val = r2_score(y_val, catboost_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for CatBoost Regressor on validation set\n",
    "print(\"Best Hyperparameters for CatBoost Regressor:\", best_params_catboost)\n",
    "print(\"CatBoost RMSE (Validation):\", catboost_rmse_val)\n",
    "print(\"CatBoost MAE (Validation):\", catboost_mae_val)\n",
    "print(\"CatBoost R-squared (Validation):\", catboost_r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc1e29",
   "metadata": {},
   "source": [
    "## 10. LightGBM Regressor HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41876fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Hyperparameters for LightGBM Regressor: OrderedDict([('learning_rate', 0.19116122853606082), ('max_depth', 4), ('n_estimators', 298)])\n",
      "LightGBM RMSE (Validation): 0.0012490768626981271\n",
      "LightGBM MAE (Validation): 0.0009665971129525779\n",
      "LightGBM R-squared (Validation): 0.9999964746174609\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_dist = {\n",
    "    'n_estimators': Integer(100, 300),\n",
    "    'max_depth': Integer(3, 5),\n",
    "    'learning_rate': Real(0.01, 0.2, prior='uniform')\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization for LightGBM Regressor\n",
    "bayes_search_lgb = BayesSearchCV(\n",
    "    LGBMRegressor(verbosity=-1),  # Set verbosity to -1 to suppress warnings\n",
    "    param_dist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_iter=10,  # Adjust the number of iterations as needed\n",
    "    random_state=42,  # Set a seed for reproducibility\n",
    "    n_jobs=-1,  # Use multiple cores for parallelization\n",
    "    verbose=1,  # Print progress\n",
    "    n_points=5,  # Number of points to sample in each iteration\n",
    "    refit=True  # Refit the best estimator with the entire dataset\n",
    ")\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for LightGBM Regressor\n",
    "best_lgb = bayes_search_lgb.best_estimator_\n",
    "best_params_lgb = bayes_search_lgb.best_params_\n",
    "\n",
    "# Predict on validation set using LightGBM Regressor\n",
    "lgb_pred_val = best_lgb.predict(X_val)\n",
    "\n",
    "# Calculate metrics for LightGBM Regressor on validation set\n",
    "lgb_rmse_val = mean_squared_error(y_val, lgb_pred_val, squared=False)\n",
    "lgb_mae_val = mean_absolute_error(y_val, lgb_pred_val)\n",
    "lgb_r2_val = r2_score(y_val, lgb_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for LightGBM Regressor on validation set\n",
    "print(\"Best Hyperparameters for LightGBM Regressor:\", best_params_lgb)\n",
    "print(\"LightGBM RMSE (Validation):\", lgb_rmse_val)\n",
    "print(\"LightGBM MAE (Validation):\", lgb_mae_val)\n",
    "print(\"LightGBM R-squared (Validation):\", lgb_r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c54a3a",
   "metadata": {},
   "source": [
    "## 11. Gradient Boosting Regressor HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65c70b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best Hyperparameters for Gradient Boosting Regressor: OrderedDict([('learning_rate', 0.19116122853606082), ('max_depth', 4), ('n_estimators', 298)])\n",
      "Gradient Boosting RMSE (Validation): 0.0011219373491899026\n",
      "Gradient Boosting MAE (Validation): 0.0008654700025997897\n",
      "Gradient Boosting R-squared (Validation): 0.9999971557672461\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_dist = {\n",
    "    'n_estimators': Integer(100, 300),\n",
    "    'max_depth': Integer(3, 5),\n",
    "    'learning_rate': Real(0.01, 0.2, prior='uniform')\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Optimization for Gradient Boosting Regressor\n",
    "bayes_search_gbr = BayesSearchCV(\n",
    "    GradientBoostingRegressor(),\n",
    "    param_dist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_iter=10,  # Adjust the number of iterations as needed\n",
    "    random_state=42,  # Set a seed for reproducibility\n",
    "    n_jobs=-1,  # Use multiple cores for parallelization\n",
    "    verbose=1,  # Print progress\n",
    "    n_points=5,  # Number of points to sample in each iteration\n",
    "    refit=True  # Refit the best estimator with the entire dataset\n",
    ")\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search_gbr.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model for Gradient Boosting Regressor\n",
    "best_gbr = bayes_search_gbr.best_estimator_\n",
    "best_params_gbr = bayes_search_gbr.best_params_\n",
    "\n",
    "# Predict on validation set using Gradient Boosting Regressor\n",
    "gbr_pred_val = best_gbr.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Gradient Boosting Regressor on validation set\n",
    "gbr_rmse_val = mean_squared_error(y_val, gbr_pred_val, squared=False)\n",
    "gbr_mae_val = mean_absolute_error(y_val, gbr_pred_val)\n",
    "gbr_r2_val = r2_score(y_val, gbr_pred_val)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Gradient Boosting Regressor on validation set\n",
    "print(\"Best Hyperparameters for Gradient Boosting Regressor:\", best_params_gbr)\n",
    "print(\"Gradient Boosting RMSE (Validation):\", gbr_rmse_val)\n",
    "print(\"Gradient Boosting MAE (Validation):\", gbr_mae_val)\n",
    "print(\"Gradient Boosting R-squared (Validation):\", gbr_r2_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb54f19e",
   "metadata": {},
   "source": [
    "## 12.Ridge Regression HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c45cbd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Ridge Regression: {'alpha': 10, 'solver': 'auto'}\n",
      "Ridge Regression RMSE: 0.5808377892754024\n",
      "Ridge Regression MAE: 0.45820805392220315\n",
      "Ridge Regression R-squared: 0.2376794115111439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define the hyperparameters and their possible values for Ridge Regression\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']} \n",
    "\n",
    "# Initialize Ridge model\n",
    "ridge = Ridge(max_iter=10000)\n",
    "\n",
    "# Initialize Grid Search\n",
    "grid_search_ridge = GridSearchCV(ridge, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Train the Grid Search\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and model for Ridge Regression\n",
    "best_ridge = grid_search_ridge.best_estimator_\n",
    "best_params_ridge = grid_search_ridge.best_params_\n",
    "\n",
    "# Predict on validation set using Ridge Regression\n",
    "ridge_pred = best_ridge.predict(X_val)\n",
    "\n",
    "# Calculate metrics for Ridge Regression\n",
    "ridge_rmse = mean_squared_error(y_val, ridge_pred, squared=False)\n",
    "ridge_mae = mean_absolute_error(y_val, ridge_pred)\n",
    "ridge_r2 = r2_score(y_val, ridge_pred)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics for Ridge Regression\n",
    "print(\"Best Hyperparameters for Ridge Regression:\", best_params_ridge)\n",
    "print(\"Ridge Regression RMSE:\", ridge_rmse)\n",
    "print(\"Ridge Regression MAE:\", ridge_mae)\n",
    "print(\"Ridge Regression R-squared:\", ridge_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a449b26e",
   "metadata": {},
   "source": [
    "# Hybrid models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff02ce53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble RMSE (Validation): 0.072622142704258\n",
      "Ensemble MAE (Validation): 0.05729186146394936\n",
      "Ensemble R-squared (Validation): 0.9880830247126653\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the weighted average ensemble\n",
    "def weighted_average_ensemble(models, weights, X):\n",
    "    predictions = np.column_stack([model.predict(X) for model in models])\n",
    "    weighted_predictions = np.sum(predictions * weights, axis=1) / np.sum(weights)\n",
    "    return weighted_predictions\n",
    "\n",
    "# Define the models and their corresponding weights for the ensemble\n",
    "models = [best_rf, best_lwlr, best_gpr, best_wknn, best_xgb, best_catboost, best_gbr, best_ridge]\n",
    "weights = [1, 1, 1, 1, 1, 1, 1, 1]  # Adjust weights as needed\n",
    "\n",
    "# Predict on the validation set using the ensemble\n",
    "ensemble_pred_val = weighted_average_ensemble(models, weights, X_val)\n",
    "\n",
    "# Calculate metrics for the ensemble on the validation set\n",
    "ensemble_rmse_val = mean_squared_error(y_val, ensemble_pred_val, squared=False)\n",
    "ensemble_mae_val = mean_absolute_error(y_val, ensemble_pred_val)\n",
    "ensemble_r2_val = r2_score(y_val, ensemble_pred_val)\n",
    "\n",
    "# Print evaluation metrics for the ensemble\n",
    "print(\"Ensemble RMSE (Validation):\", ensemble_rmse_val)\n",
    "print(\"Ensemble MAE (Validation):\", ensemble_mae_val)\n",
    "print(\"Ensemble R-squared (Validation):\", ensemble_r2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fb90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weights for each model\n",
    "weight_rf = 0.2\n",
    "weight_lwlr = 0.2\n",
    "weight_poly_kernel = 0.1\n",
    "weight_rbf_kernel = 0.1\n",
    "weight_gpr = 0.1\n",
    "weight_wknn = 0.1\n",
    "weight_knn = 0.1\n",
    "weight_xgb = 0.05\n",
    "weight_catboost = 0.05\n",
    "weight_lgb = 0.05\n",
    "weight_gbr = 0.05\n",
    "weight_ridge = 0.05\n",
    "\n",
    "# Create hybrid predictions\n",
    "hybrid_pred_val = (\n",
    "    weight_rf * rf_pred_val +\n",
    "    weight_lwlr * lwlr_pred_val +\n",
    "    weight_poly_kernel * poly_kernel_pred_val +\n",
    "    weight_rbf_kernel * rbf_kernel_pred_val +\n",
    "    weight_gpr * gpr_pred_val +\n",
    "    weight_wknn * wknn_pred_val +\n",
    "    weight_knn * knn_pred_val +\n",
    "    weight_xgb * xgb_pred_val +\n",
    "    weight_catboost * catboost_pred_val +\n",
    "    weight_lgb * lgb_pred_val +\n",
    "    weight_gbr * gbr_pred_val +\n",
    "    weight_ridge * ridge_pred_val\n",
    ")\n",
    "\n",
    "# Calculate metrics for the hybrid model on validation set\n",
    "hybrid_rmse_val = mean_squared_error(y_val, hybrid_pred_val, squared=False)\n",
    "hybrid_mae_val = mean_absolute_error(y_val, hybrid_pred_val)\n",
    "hybrid_r2_val = r2_score(y_val, hybrid_pred_val)\n",
    "\n",
    "# Print evaluation metrics for the hybrid model on validation set\n",
    "print(\"Hybrid RMSE (Validation):\", hybrid_rmse_val)\n",
    "print(\"Hybrid MAE (Validation):\", hybrid_mae_val)\n",
    "print(\"Hybrid R-squared (Validation):\", hybrid_r2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79aeb4d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpr_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 20\u001b[0m\n\u001b[0;32m     14\u001b[0m weight_ridge \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create hybrid predictions\u001b[39;00m\n\u001b[0;32m     17\u001b[0m hybrid1_pred \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     18\u001b[0m     weight_lwlr \u001b[38;5;241m*\u001b[39m lwlr_pred \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     19\u001b[0m     weight_ridge \u001b[38;5;241m*\u001b[39m ridge_pred \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m---> 20\u001b[0m     weight_gpr \u001b[38;5;241m*\u001b[39m \u001b[43mgpr_pred\u001b[49m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     21\u001b[0m     weight_wknn \u001b[38;5;241m*\u001b[39m wknn_pred \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     22\u001b[0m     weight_xgb \u001b[38;5;241m*\u001b[39m xgb_pred \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     23\u001b[0m     weight_catboost \u001b[38;5;241m*\u001b[39m catboost_pred \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     24\u001b[0m     weight_gbr \u001b[38;5;241m*\u001b[39m gbr_pred \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     25\u001b[0m     weight_ridge \u001b[38;5;241m*\u001b[39m ridge_pred\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of Hybrid Model 1\u001b[39;00m\n\u001b[0;32m     29\u001b[0m hybrid1_rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, hybrid1_pred, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gpr_pred' is not defined"
     ]
    }
   ],
   "source": [
    "#best_rf, best_lwlr, best_gpr, best_wknn, best_xgb, best_catboost, best_gbr, best_ridge\n",
    "\n",
    "# Assuming you already have predictions for all the models\n",
    "# lwlr_pred, ridge_pred, gpr_pred, wknn_pred, xgb_pred, catboost_pred, gbr_pred, ridge_pred\n",
    "\n",
    "# Define the weights for each model\n",
    "weight_lwlr = 0.15\n",
    "weight_ridge = 0.15\n",
    "weight_gpr = 0.1\n",
    "weight_wknn = 0.1\n",
    "weight_xgb = 0.1\n",
    "weight_catboost = 0.1\n",
    "weight_gbr = 0.1\n",
    "weight_ridge = 0.1\n",
    "\n",
    "# Create hybrid predictions\n",
    "hybrid1_pred = (\n",
    "    weight_lwlr * lwlr_pred +\n",
    "    weight_ridge * ridge_pred +\n",
    "    weight_gpr * gpr_pred +\n",
    "    weight_wknn * wknn_pred +\n",
    "    weight_xgb * xgb_pred +\n",
    "    weight_catboost * catboost_pred +\n",
    "    weight_gbr * gbr_pred +\n",
    "    weight_ridge * ridge_pred\n",
    ")\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 1\n",
    "hybrid1_rmse = mean_squared_error(y_val, hybrid1_pred, squared=False)\n",
    "hybrid1_mae = mean_absolute_error(y_val, hybrid1_pred)\n",
    "hybrid1_r2 = r2_score(y_val, hybrid1_pred)\n",
    "\n",
    "print(\"Hybrid Model 1 (LWLR + Ridge + GPR + WKNN + XGB + CatBoost + GBR + Ridge) RMSE:\", hybrid1_rmse)\n",
    "print(\"Hybrid Model 1 (LWLR + Ridge + GPR + WKNN + XGB + CatBoost + GBR + Ridge) MAE:\", hybrid1_mae)\n",
    "print(\"Hybrid Model 1 (LWLR + Ridge + GPR + WKNN + XGB + CatBoost + GBR + Ridge) R-squared:\", hybrid1_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a6e7bde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_krr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m lwlr_wknn_pred_val \u001b[38;5;241m=\u001b[39m lwlr_pred_val \u001b[38;5;241m*\u001b[39m wknn_pred_val\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming you have predictions for best_krr\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m hybrid2_pred_val \u001b[38;5;241m=\u001b[39m \u001b[43mbest_krr\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_val) \u001b[38;5;241m+\u001b[39m lwlr_wknn_pred_val\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of Hybrid Model 2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m hybrid2_rmse_val \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, hybrid2_pred_val, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_krr' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you already have predictions for lwlr_pred_val and wknn_pred_val\n",
    "lwlr_wknn_pred_val = lwlr_pred_val * wknn_pred_val\n",
    "\n",
    "# Assuming you have predictions for best_krr\n",
    "hybrid2_pred_val = best_krr.predict(X_val) + lwlr_wknn_pred_val\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 2\n",
    "hybrid2_rmse_val = mean_squared_error(y_val, hybrid2_pred_val, squared=False)\n",
    "hybrid2_mae_val = mean_absolute_error(y_val, hybrid2_pred_val)\n",
    "hybrid2_r2_val = r2_score(y_val, hybrid2_pred_val)\n",
    "\n",
    "print(\"Hybrid Model 2 (KRR + (LWLR * Weighted K-NN)) RMSE:\", hybrid2_rmse_val)\n",
    "print(\"Hybrid Model 2 (KRR + (LWLR * Weighted K-NN)) MAE:\", hybrid2_mae_val)\n",
    "print(\"Hybrid Model 2 (KRR + (LWLR * Weighted K-NN)) R-squared:\", hybrid2_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbe890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for krr_pred, lwlr_pred_val, and wknn_pred_val\n",
    "hybrid3_pred_val = krr_pred + lwlr_pred_val + wknn_pred_val\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 3\n",
    "hybrid3_rmse_val = mean_squared_error(y_val, hybrid3_pred_val, squared=False)\n",
    "hybrid3_mae_val = mean_absolute_error(y_val, hybrid3_pred_val)\n",
    "hybrid3_r2_val = r2_score(y_val, hybrid3_pred_val)\n",
    "\n",
    "print(\"Hybrid Model 3 (KRR (Ensemble) + LWLR + Weighted K-NN) RMSE:\", hybrid3_rmse_val)\n",
    "print(\"Hybrid Model 3 (KRR (Ensemble) + LWLR + Weighted K-NN) MAE:\", hybrid3_mae_val)\n",
    "print(\"Hybrid Model 3 (KRR (Ensemble) + LWLR + Weighted K-NN) R-squared:\", hybrid3_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02be3a08",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'krr_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you already have predictions for krr_pred and lwlr_pred_val\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hybrid4_pred_val \u001b[38;5;241m=\u001b[39m (\u001b[43mkrr_pred\u001b[49m \u001b[38;5;241m+\u001b[39m lwlr_pred_val) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of Hybrid Model 4\u001b[39;00m\n\u001b[0;32m      5\u001b[0m hybrid4_rmse_val \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, hybrid4_pred_val, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'krr_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you already have predictions for krr_pred and lwlr_pred_val\n",
    "hybrid4_pred_val = (krr_pred + lwlr_pred_val) / 2\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 4\n",
    "hybrid4_rmse_val = mean_squared_error(y_val, hybrid4_pred_val, squared=False)\n",
    "hybrid4_mae_val = mean_absolute_error(y_val, hybrid4_pred_val)\n",
    "hybrid4_r2_val = r2_score(y_val, hybrid4_pred_val)\n",
    "\n",
    "print(\"Hybrid Model 4 (KRR + LWLR) RMSE:\", hybrid4_rmse_val)\n",
    "print(\"Hybrid Model 4 (KRR + LWLR) MAE:\", hybrid4_mae_val)\n",
    "print(\"Hybrid Model 4 (KRR + LWLR) R-squared:\", hybrid4_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7be35b46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'krr_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you already have predictions for krr_pred, lwlr_pred_val, and wknn_pred_val\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hybrid5_pred_val \u001b[38;5;241m=\u001b[39m (\u001b[43mkrr_pred\u001b[49m \u001b[38;5;241m+\u001b[39m lwlr_pred_val \u001b[38;5;241m+\u001b[39m wknn_pred_val) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of Hybrid Model 5\u001b[39;00m\n\u001b[0;32m      5\u001b[0m hybrid5_rmse_val \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, hybrid5_pred_val, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'krr_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you already have predictions for krr_pred, lwlr_pred_val, and wknn_pred_val\n",
    "hybrid5_pred_val = (krr_pred + lwlr_pred_val + wknn_pred_val) / 3\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 5\n",
    "hybrid5_rmse_val = mean_squared_error(y_val, hybrid5_pred_val, squared=False)\n",
    "hybrid5_mae_val = mean_absolute_error(y_val, hybrid5_pred_val)\n",
    "hybrid5_r2_val = r2_score(y_val, hybrid5_pred_val)\n",
    "\n",
    "print(\"Hybrid Model 5 (KRR + LWLR + Weighted K-NN) RMSE:\", hybrid5_rmse_val)\n",
    "print(\"Hybrid Model 5 (KRR + LWLR + Weighted K-NN) MAE:\", hybrid5_mae_val)\n",
    "print(\"Hybrid Model 5 (KRR + LWLR + Weighted K-NN) R-squared:\", hybrid5_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for rf_pred_val and lwlr_pred_val\n",
    "hybrid6_pred_val = (rf_pred_val + lwlr_pred_val) / 2\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 6\n",
    "hybrid6_rmse_val = mean_squared_error(y_val, hybrid6_pred_val, squared=False)\n",
    "hybrid6_mae_val = mean_absolute_error(y_val, hybrid6_pred_val)\n",
    "hybrid6_r2_val = r2_score(y_val, hybrid6_pred_val)\n",
    "\n",
    "print(\"Hybrid Model 6 (Random Forest + LWLR) RMSE:\", hybrid6_rmse_val)\n",
    "print(\"Hybrid Model 6 (Random Forest + LWLR) MAE:\", hybrid6_mae_val)\n",
    "print(\"Hybrid Model 6 (Random Forest + LWLR) R-squared:\", hybrid6_r2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68dddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have predictions for rf_pred_val and lwlr_pred_val\n",
    "hybrid1_pred = (weight_rf * rf_pred_val + weight_lwlr * lwlr_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 1 on the validation set\n",
    "hybrid1_rmse = mean_squared_error(y_val, hybrid1_pred, squared=False)\n",
    "hybrid1_mae = mean_absolute_error(y_val, hybrid1_pred)\n",
    "hybrid1_r2 = r2_score(y_val, hybrid1_pred)\n",
    "\n",
    "print(\"Hybrid Model 1 (RF + LWLR) RMSE:\", hybrid1_rmse)\n",
    "print(\"Hybrid Model 1 (RF + LWLR) MAE:\", hybrid1_mae)\n",
    "print(\"Hybrid Model 1 (RF + LWLR) R-squared:\", hybrid1_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee458472",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weight_poly_kernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you already have predictions for poly_kernel_pred_val and rbf_kernel_pred_val\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hybrid2_pred \u001b[38;5;241m=\u001b[39m (\u001b[43mweight_poly_kernel\u001b[49m \u001b[38;5;241m*\u001b[39m poly_kernel_pred_val \u001b[38;5;241m+\u001b[39m weight_rbf_kernel \u001b[38;5;241m*\u001b[39m rbf_kernel_pred_val)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of Hybrid Model 2 on the validation set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m hybrid2_rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, hybrid2_pred, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weight_poly_kernel' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you already have predictions for poly_kernel_pred_val and rbf_kernel_pred_val\n",
    "hybrid2_pred = (weight_poly_kernel * poly_kernel_pred_val + weight_rbf_kernel * rbf_kernel_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 2 on the validation set\n",
    "hybrid2_rmse = mean_squared_error(y_val, hybrid2_pred, squared=False)\n",
    "hybrid2_mae = mean_absolute_error(y_val, hybrid2_pred)\n",
    "hybrid2_r2 = r2_score(y_val, hybrid2_pred)\n",
    "\n",
    "print(\"Hybrid Model 2 (KRR (Poly Kernel) + RBF Kernel) RMSE:\", hybrid2_rmse)\n",
    "print(\"Hybrid Model 2 (KRR (Poly Kernel) + RBF Kernel) MAE:\", hybrid2_mae)\n",
    "print(\"Hybrid Model 2 (KRR (Poly Kernel) + RBF Kernel) R-squared:\", hybrid2_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39e18e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model 3 (XGBoost + CatBoost) RMSE: 2.5342233166727732\n",
      "Hybrid Model 3 (XGBoost + CatBoost) MAE: 2.4777081149336446\n",
      "Hybrid Model 3 (XGBoost + CatBoost) R-squared: -13.511679778229412\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have predictions for xgb_pred_val and catboost_pred_val\n",
    "hybrid3_pred = (weight_xgb * xgb_pred_val + weight_catboost * catboost_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 3 on the validation set\n",
    "hybrid3_rmse = mean_squared_error(y_val, hybrid3_pred, squared=False)\n",
    "hybrid3_mae = mean_absolute_error(y_val, hybrid3_pred)\n",
    "hybrid3_r2 = r2_score(y_val, hybrid3_pred)\n",
    "\n",
    "print(\"Hybrid Model 3 (XGBoost + CatBoost) RMSE:\", hybrid3_rmse)\n",
    "print(\"Hybrid Model 3 (XGBoost + CatBoost) MAE:\", hybrid3_mae)\n",
    "print(\"Hybrid Model 3 (XGBoost + CatBoost) R-squared:\", hybrid3_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b96557e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weight_lgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you already have predictions for lgb_pred_val and gbr_pred_val\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hybrid4_pred \u001b[38;5;241m=\u001b[39m (\u001b[43mweight_lgb\u001b[49m \u001b[38;5;241m*\u001b[39m lgb_pred_val \u001b[38;5;241m+\u001b[39m weight_gbr \u001b[38;5;241m*\u001b[39m gbr_pred_val)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of Hybrid Model 4 on the validation set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m hybrid4_rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, hybrid4_pred, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weight_lgb' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you already have predictions for lgb_pred_val and gbr_pred_val\n",
    "hybrid4_pred = (weight_lgb * lgb_pred_val + weight_gbr * gbr_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 4 on the validation set\n",
    "hybrid4_rmse = mean_squared_error(y_val, hybrid4_pred, squared=False)\n",
    "hybrid4_mae = mean_absolute_error(y_val, hybrid4_pred)\n",
    "hybrid4_r2 = r2_score(y_val, hybrid4_pred)\n",
    "\n",
    "print(\"Hybrid Model 4 (LightGBM + Gradient Boosting) RMSE:\", hybrid4_rmse)\n",
    "print(\"Hybrid Model 4 (LightGBM + Gradient Boosting) MAE:\", hybrid4_mae)\n",
    "print(\"Hybrid Model 4 (LightGBM + Gradient Boosting) R-squared:\", hybrid4_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f831856",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (969770107.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[46], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"Hybrid Model 6 (RF + LWLR + XGBoost) RMSE:\", hybrid6_rmse\u001b[0m\n\u001b[1;37m                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have predictions for rf_pred_val, lwlr_pred_val, and xgb_pred_val\n",
    "hybrid6_pred = (weight_rf * rf_pred_val + weight_lwlr * lwlr_pred_val + weight_xgb * xgb_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 6 on the validation set\n",
    "hybrid6_rmse = mean_squared_error(y_val, hybrid6_pred, squared=False)\n",
    "hybrid6_mae = mean_absolute_error(y_val, hybrid6_pred)\n",
    "hybrid6_r2 = r2_score(y_val, hybrid6_pred)\n",
    "\n",
    "print(\"Hybrid Model 6 (RF + LWLR + XGBoost) RMSE:\", hybrid6_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d374e43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weight_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you have rf_pred_val and lwlr_pred_val\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hybrid1_pred \u001b[38;5;241m=\u001b[39m (\u001b[43mweight_rf\u001b[49m \u001b[38;5;241m*\u001b[39m rf_pred_val \u001b[38;5;241m+\u001b[39m weight_lwlr \u001b[38;5;241m*\u001b[39m lwlr_pred_val)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of Hybrid Model 1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m hybrid1_rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, hybrid1_pred, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weight_rf' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you have rf_pred_val and lwlr_pred_val\n",
    "hybrid1_pred = (weight_rf * rf_pred_val + weight_lwlr * lwlr_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 1\n",
    "hybrid1_rmse = mean_squared_error(y_val, hybrid1_pred, squared=False)\n",
    "hybrid1_mae = mean_absolute_error(y_val, hybrid1_pred)\n",
    "hybrid1_r2 = r2_score(y_val, hybrid1_pred)\n",
    "\n",
    "print(\"Hybrid Model 1 (RF + LWLR) RMSE:\", hybrid1_rmse)\n",
    "print(\"Hybrid Model 1 (RF + LWLR) MAE:\", hybrid1_mae)\n",
    "print(\"Hybrid Model 1 (RF + LWLR) R-squared:\", hybrid1_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7272d03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weight_lgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you have lgb_pred_val and catboost_pred_val\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hybrid3_pred \u001b[38;5;241m=\u001b[39m (\u001b[43mweight_lgb\u001b[49m \u001b[38;5;241m*\u001b[39m lgb_pred_val \u001b[38;5;241m+\u001b[39m weight_catboost \u001b[38;5;241m*\u001b[39m catboost_pred_val)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of Hybrid Model 3\u001b[39;00m\n\u001b[0;32m      5\u001b[0m hybrid3_rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, hybrid3_pred, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weight_lgb' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you have lgb_pred_val and catboost_pred_val\n",
    "hybrid3_pred = (weight_lgb * lgb_pred_val + weight_catboost * catboost_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 3\n",
    "hybrid3_rmse = mean_squared_error(y_val, hybrid3_pred, squared=False)\n",
    "hybrid3_mae = mean_absolute_error(y_val, hybrid3_pred)\n",
    "hybrid3_r2 = r2_score(y_val, hybrid3_pred)\n",
    "\n",
    "print(\"Hybrid Model 3 (LightGBM + CatBoost) RMSE:\", hybrid3_rmse)\n",
    "print(\"Hybrid Model 3 (LightGBM + CatBoost) MAE:\", hybrid3_mae)\n",
    "print(\"Hybrid Model 3 (LightGBM + CatBoost) R-squared:\", hybrid3_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83649c64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weight_poly_kernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you have ridge_pred and poly_kernel_pred_val\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hybrid5_pred \u001b[38;5;241m=\u001b[39m (weight_ridge \u001b[38;5;241m*\u001b[39m ridge_pred \u001b[38;5;241m+\u001b[39m \u001b[43mweight_poly_kernel\u001b[49m \u001b[38;5;241m*\u001b[39m poly_kernel_pred_val)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of Hybrid Model 5\u001b[39;00m\n\u001b[0;32m      5\u001b[0m hybrid5_rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, hybrid5_pred, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weight_poly_kernel' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you have ridge_pred and poly_kernel_pred_val\n",
    "hybrid5_pred = (weight_ridge * ridge_pred + weight_poly_kernel * poly_kernel_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 5\n",
    "hybrid5_rmse = mean_squared_error(y_val, hybrid5_pred, squared=False)\n",
    "hybrid5_mae = mean_absolute_error(y_val, hybrid5_pred)\n",
    "hybrid5_r2 = r2_score(y_val, hybrid5_pred)\n",
    "\n",
    "print(\"Hybrid Model 5 (Ridge + Poly Kernel) RMSE:\", hybrid5_rmse)\n",
    "print(\"Hybrid Model 5 (Ridge + Poly Kernel) MAE:\", hybrid5_mae)\n",
    "print(\"Hybrid Model 5 (Ridge + Poly Kernel) R-squared:\", hybrid5_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78b19c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model 7 (RF + LightGBM + CatBoost) RMSE: 1.0037832889414366e-05\n",
      "Hybrid Model 7 (RF + LightGBM + CatBoost) MAE: 7.617003841855568e-06\n",
      "Hybrid Model 7 (RF + LightGBM + CatBoost) R-squared: 0.9999999997723289\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have rf_pred_val, lgb_pred_val, and catboost_pred_val\n",
    "\n",
    "# Define weights for models (you can adjust these)\n",
    "weight_rf = 0.4\n",
    "weight_lgb = 0.3\n",
    "weight_catboost = 0.3\n",
    "\n",
    "# Combine predictions\n",
    "hybrid7_pred = (weight_rf * rf_pred_val + weight_lgb * lgb_pred_val + weight_catboost * catboost_pred_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 7\n",
    "hybrid7_rmse = mean_squared_error(y_val, hybrid7_pred, squared=False)\n",
    "hybrid7_mae = mean_absolute_error(y_val, hybrid7_pred)\n",
    "hybrid7_r2 = r2_score(y_val, hybrid7_pred)\n",
    "\n",
    "print(\"Hybrid Model 7 (RF + LightGBM + CatBoost) RMSE:\", hybrid7_rmse)\n",
    "print(\"Hybrid Model 7 (RF + LightGBM + CatBoost) MAE:\", hybrid7_mae)\n",
    "print(\"Hybrid Model 7 (RF + LightGBM + CatBoost) R-squared:\", hybrid7_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a08eea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model 8 (Stacking Ensemble) RMSE: 2.0090405694587458e-05\n",
      "Hybrid Model 8 (Stacking Ensemble) MAE: 1.5135989370236077e-05\n",
      "Hybrid Model 8 (Stacking Ensemble) R-squared: 0.999999999087978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming you have xgb_pred_val, gbr_pred_val, and lgb_pred_val\n",
    "\n",
    "# Initialize the Meta-model (you can choose a different one if desired)\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "# Create the training set for the meta-model\n",
    "meta_X_train = np.column_stack((xgb_pred_val, gbr_pred_val, lgb_pred_val))\n",
    "\n",
    "# Train the Meta-model\n",
    "meta_model.fit(meta_X_train, y_val)\n",
    "\n",
    "# Create the test set for the meta-model\n",
    "meta_X_val = np.column_stack((xgb_pred_val, gbr_pred_val, lgb_pred_val))\n",
    "\n",
    "# Predict using the Meta-model\n",
    "hybrid8_pred = meta_model.predict(meta_X_val)\n",
    "\n",
    "# Evaluate the performance of Hybrid Model 8\n",
    "hybrid8_rmse = mean_squared_error(y_val, hybrid8_pred, squared=False)\n",
    "hybrid8_mae = mean_absolute_error(y_val, hybrid8_pred)\n",
    "hybrid8_r2 = r2_score(y_val, hybrid8_pred)\n",
    "\n",
    "print(\"Hybrid Model 8 (Stacking Ensemble) RMSE:\", hybrid8_rmse)\n",
    "print(\"Hybrid Model 8 (Stacking Ensemble) MAE:\", hybrid8_mae)\n",
    "print(\"Hybrid Model 8 (Stacking Ensemble) R-squared:\", hybrid8_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "927a7147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average RMSE (Validation): 0.6231582545497936\n",
      "Weighted Average MAE (Validation): 0.6185810176715901\n",
      "Weighted Average R-squared (Validation): 0.12254545870019329\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have predictions from all your tuned models stored in variables\n",
    "# rf_pred_val, lwlr_pred_val, poly_kernel_pred_val, rbf_kernel_pred_val, gpr_pred_val, wknn_pred_val, knn_pred_val, xgb_pred_val, catboost_pred_val, lgb_pred_val, gbr_pred_val, ridge_pred_val\n",
    "\n",
    "# Define weights for each model (you can adjust these)\n",
    "weights = {\n",
    "    'rf': 0.1,\n",
    "    'lwlr': 0.1,\n",
    "    'poly_kernel': 0.1,\n",
    "    'rbf_kernel': 0.1,\n",
    "    'gpr': 0.1,\n",
    "    'wknn': 0.1,\n",
    "    'knn': 0.1,\n",
    "    'xgb': 0.1,\n",
    "    'catboost': 0.1,\n",
    "    'lgb': 0.1,\n",
    "    'gbr': 0.1,\n",
    "    'ridge': 0.1\n",
    "}\n",
    "\n",
    "# List of model predictions\n",
    "predictions = [\n",
    "    rf_pred_val, lwlr_pred_val, poly_kernel_pred_val, rbf_kernel_pred_val,\n",
    "    gpr_pred_val, wknn_pred_val, knn_pred_val, xgb_pred_val, catboost_pred_val,\n",
    "    lgb_pred_val, gbr_pred_val, ridge_pred_val\n",
    "]\n",
    "\n",
    "# Initialize an array to store the weighted predictions\n",
    "weighted_predictions = np.zeros_like(rf_pred_val)\n",
    "\n",
    "# Combine predictions with weights\n",
    "for model_pred, weight in zip(predictions, weights.values()):\n",
    "    weighted_predictions += model_pred * weight\n",
    "\n",
    "# Calculate metrics for the weighted predictions\n",
    "weighted_rmse_val = mean_squared_error(y_val, weighted_predictions, squared=False)\n",
    "weighted_mae_val = mean_absolute_error(y_val, weighted_predictions)\n",
    "weighted_r2_val = r2_score(y_val, weighted_predictions)\n",
    "\n",
    "# Print the evaluation metrics for the weighted predictions\n",
    "print(\"Weighted Average RMSE (Validation):\", weighted_rmse_val)\n",
    "print(\"Weighted Average MAE (Validation):\", weighted_mae_val)\n",
    "print(\"Weighted Average R-squared (Validation):\", weighted_r2_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_test, y_test, and all tuned models are defined\n",
    "\n",
    "# Get predictions for all tuned models on the test set\n",
    "rf_test_pred = best_rf.predict(X_test)\n",
    "lwlr_test_pred = best_lwlr.predict(X_test)\n",
    "poly_kernel_test_pred = best_poly_kernel.predict(X_test)\n",
    "rbf_kernel_test_pred = best_rbf_kernel.predict(X_test)\n",
    "gpr_test_pred = best_gpr.predict(X_test)\n",
    "wknn_test_pred = best_wknn.predict(X_test)\n",
    "knn_test_pred = best_knn.predict(X_test)\n",
    "xgb_test_pred = best_xgb.predict(X_test)\n",
    "catboost_test_pred = best_catboost.predict(X_test)\n",
    "lgb_test_pred = best_lgb.predict(X_test)\n",
    "gbr_test_pred = best_gbr.predict(X_test)\n",
    "ridge_test_pred = best_ridge.predict(X_test)\n",
    "\n",
    "# Define weights for each model's predictions\n",
    "weights = {\n",
    "    'rf': 0.05,\n",
    "    'lwlr': 0.05,\n",
    "    'poly_kernel': 0.1,\n",
    "    'rbf_kernel': 0.1,\n",
    "    'gpr': 0.05,\n",
    "    'wknn': 0.05,\n",
    "    'knn': 0.05,\n",
    "    'xgb': 0.1,\n",
    "    'catboost': 0.1,\n",
    "    'lgb': 0.1,\n",
    "    'gbr': 0.1,\n",
    "    'ridge': 0.1\n",
    "}\n",
    "\n",
    "# Combine predictions using weighted averaging\n",
    "hybrid_complex_test_pred = (\n",
    "    weights['rf']*rf_test_pred + weights['lwlr']*lwlr_test_pred +\n",
    "    weights['poly_kernel']*poly_kernel_test_pred + weights['rbf_kernel']*rbf_kernel_test_pred +\n",
    "    weights['gpr']*gpr_test_pred + weights['wknn']*wknn_test_pred +\n",
    "    weights['knn']*knn_test_pred + weights['xgb']*xgb_test_pred +\n",
    "    weights['catboost']*catboost_test_pred + weights['lgb']*lgb_test_pred +\n",
    "    weights['gbr']*gbr_test_pred + weights['ridge']*ridge_test_pred\n",
    ")\n",
    "\n",
    "# Calculate evaluation metrics for the complex hybrid model\n",
    "hybrid_complex_test_rmse = mean_squared_error(y_test, hybrid_complex_test_pred, squared=False)\n",
    "hybrid_complex_test_mae = mean_absolute_error(y_test, hybrid_complex_test_pred)\n",
    "hybrid_complex_test_r2 = r2_score(y_test, hybrid_complex_test_pred)\n",
    "\n",
    "# Print the evaluation metrics for the complex hybrid model\n",
    "print(\"Complex Hybrid Model Test RMSE:\", hybrid_complex_test_rmse)\n",
    "print(\"Complex Hybrid Model Test MAE:\", hybrid_complex_test_mae)\n",
    "print(\"Complex Hybrid Model Test R-squared:\", hybrid_complex_test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c184bfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model 5 (Random Forest + Ridge + KNN + ElasticNet) RMSE: 0.11616755785508046\n",
      "Hybrid Model 5 (Random Forest + Ridge + KNN + ElasticNet) MAE: 0.09164161078444068\n",
      "Hybrid Model 5 (Random Forest + Ridge + KNN ) R-squared: 0.9695071764604458\n"
     ]
    }
   ],
   "source": [
    "# Predictions from Random Forest, Ridge, KNN, and ElasticNet\n",
    "rf_pred = best_rf.predict(X_val)\n",
    "ridge_pred = best_ridge.predict(X_val)\n",
    "knn_pred = best_knn.predict(X_val)\n",
    "lwlr_pred = best_lwlr.predict(X_val)\n",
    "\n",
    "# Define weights for Random Forest, Ridge, KNN, and ElasticNet predictions\n",
    "weight_rf = 0.3  # Adjust as needed\n",
    "weight_ridge = 0.2\n",
    "weight_knn = 0.2\n",
    "weight_lwlr = 0.3\n",
    "\n",
    "hybrid5_pred = (weight_rf * rf_pred + weight_ridge * ridge_pred + weight_knn * knn_pred + weight_lwlr * lwlr_pred)\n",
    "\n",
    "\n",
    "hybrid5_rmse = mean_squared_error(y_val, hybrid5_pred, squared=False)\n",
    "hybrid5_mae = mean_absolute_error(y_val, hybrid5_pred)\n",
    "hybrid5_r2 = r2_score(y_val, hybrid5_pred)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Hybrid Model 5 (Random Forest + Ridge + KNN + ElasticNet) RMSE:\", hybrid5_rmse)\n",
    "print(\"Hybrid Model 5 (Random Forest + Ridge + KNN + ElasticNet) MAE:\", hybrid5_mae)\n",
    "print(\"Hybrid Model 5 (Random Forest + Ridge + KNN ) R-squared:\", hybrid5_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513aee4",
   "metadata": {},
   "source": [
    "# Testing nusing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df.loc[(df['Date'].dt.year >= 1993) & (df['Date'].dt.year <= 2012)].drop(columns=['Rangpur-27 Satgora Mistripara (Rangpur Sadar)', 'Date']).to_numpy()\n",
    "y_test = df.loc[(df['Date'].dt.year >= 1993) & (df['Date'].dt.year <= 2012)]['Rangpur-27 Satgora Mistripara (Rangpur Sadar)'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7393c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c633e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e39b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_val is the actual target values and hybrid1_pred is the predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_val, hybrid1_pred, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values (Validation Set)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# For the test set\n",
    "# Assuming y_test is the actual target values and hybrid1_test_pred is the predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, hybrid1_test_pred, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values (Test Set)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df063dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
